{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f28c93-e932-40b2-94df-d6f9dd383f29",
   "metadata": {},
   "source": [
    "# Классификатор купит / не купит в течение трех дней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7ba916a-4d76-4c6b-a364-ee4d6a30964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Загрузка данных\n",
    "def load_data():\n",
    "    catalog = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\stokman_catalog_preprocessed.pq')\n",
    "    actions = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\train_actions.pq')\n",
    "    vector_mapping = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\catalog_vector_mapping.pq')\n",
    "    vectors = np.load('C:\\\\Users\\\\nikit\\\\Hackaton\\\\vectors.npz')['arr_0']  # Извлечение эмбеддингов товаров\n",
    "\n",
    "    return catalog, actions, vector_mapping, vectors\n",
    "\n",
    "# Предобработка данных\n",
    "def preprocess_data(actions, catalog, vector_mapping):\n",
    "    # Преобразование даты\n",
    "    actions['date'] = pd.to_datetime(actions['date'])\n",
    "    \n",
    "    # Присоединение каталога товаров\n",
    "    actions = actions.explode('products')  # Распаковка массива products\n",
    "    actions = actions.rename(columns={'products': 'product_id'})\n",
    "    actions = actions.merge(catalog[['product_id', 'price', 'category_id']], on='product_id', how='left')\n",
    "    \n",
    "    # Присоединение векторов товаров\n",
    "    actions = actions.merge(vector_mapping, on='product_id', how='left')\n",
    "    \n",
    "    return actions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    catalog, actions, vector_mapping, vectors = load_data()\n",
    "    actions = preprocess_data(actions, catalog, vector_mapping)\n",
    "    actions.to_parquet('actions_preprocessed.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b84f761-6b98-4911-a490-7287e0f3c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Генерация временных и взаимодействующих признаков\n",
    "def generate_features(actions):\n",
    "    # Время с последнего действия для каждого пользователя\n",
    "    actions['days_since_last_action'] = actions.groupby('user_id')['date'].diff().dt.days\n",
    "    \n",
    "    # Признаки активности за последние 3 дня\n",
    "    def count_recent_actions(df, days):\n",
    "        recent = df[df['date'] >= df['date'].max() - pd.Timedelta(days=days)]\n",
    "        return recent.groupby('user_id')['action'].count()\n",
    "    \n",
    "    recent_activity_3d = count_recent_actions(actions, 3)\n",
    "    \n",
    "    # Количество покупок, добавлений в корзину и просмотров\n",
    "    agg_features = actions.groupby('user_id').agg({\n",
    "        'product_id': 'nunique',  # Количество уникальных товаров\n",
    "        'price': 'mean',          # Средняя цена товаров\n",
    "        'action': ['count', lambda x: (x == 5).sum()],  # Количество действий и покупок\n",
    "        'days_since_last_action': 'min'  # Время с последнего действия\n",
    "    }).reset_index()\n",
    "    \n",
    "    agg_features.columns = ['user_id', 'n_unique_products', 'avg_price', 'n_actions', 'n_orders', 'days_since_last_action']\n",
    "    \n",
    "    # Объединение с активностью за 3 дня\n",
    "    agg_features = agg_features.merge(recent_activity_3d, on='user_id', how='left')\n",
    "    agg_features = agg_features.rename(columns={'action': 'actions_last_3d'})\n",
    "    \n",
    "    return agg_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actions = pd.read_parquet('actions_preprocessed.pq')\n",
    "    user_features = generate_features(actions)\n",
    "    \n",
    "    user_features.to_parquet('user_features.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c904f48d-17fd-4b5d-aaa4-180c29d70246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сгенерированные фичи для предсказания покупки:\n",
    "# Средняя цена интересующих товаров - avg_price\n",
    "# Время с последней активности - days_since_last_action\n",
    "# Общее количество действий - n_actions\n",
    "# Количество покупок - n_orders\n",
    "# Количество уникальных товаров - n_unique_products\n",
    "# Действия за последние три дня - actions_last_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0cdf3ced-ff30-4fa0-9949-13fd99f91f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Предобработка отсутствующих значений\n",
    "def null_data_preprocessing(user_features):\n",
    "    user_features.fillna(0)\n",
    "    user_features.to_parquet('user_features.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a8c7d77c-e05d-4fce-96dd-3af8d23fae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2654, number of negative: 372894\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 942\n",
      "[LightGBM] [Info] Number of data points in the train set: 375548, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007067 -> initscore=-4.945226\n",
      "[LightGBM] [Info] Start training from score -4.945226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     93250\n",
      "           1       0.51      0.20      0.29       637\n",
      "\n",
      "    accuracy                           0.99     93887\n",
      "   macro avg       0.75      0.60      0.64     93887\n",
      "weighted avg       0.99      0.99      0.99     93887\n",
      "\n",
      "Wirh accuracy: 0.9932685036267002\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "# import mlflow\n",
    "# import mlflow.lightgbm\n",
    "\n",
    "# Загрузка данных\n",
    "def load_features_data():\n",
    "    features = pd.read_parquet('user_features.pq')\n",
    "    return features\n",
    "\n",
    "# Обучение модели\n",
    "def train_model(features):\n",
    "    # Целевая переменная: наличие заказов\n",
    "    features = features.drop('user_id', axis=1)\n",
    "    \n",
    "    X = features.drop(columns=['n_orders'])  # Признаки\n",
    "    y = (features['n_orders'] > 0).astype(int)  # Купил ли товар\n",
    "    \n",
    "    # Разделение на тренировочные и тестовые данные\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "    \n",
    "    # Параметры модели\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "    \n",
    "    # # Логирование через MLFlow\n",
    "    # mlflow.lightgbm.autolog()\n",
    "    \n",
    "    # with mlflow.start_run():\n",
    "    model = lgb.train(params, train_data, valid_sets=[valid_data], num_boost_round=100)\n",
    "    \n",
    "    # Предсказания и метрики\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # mlflow.log_metric(\"precision\", precision)\n",
    "    # mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'Wirh accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = load_features_data()\n",
    "    null_data_preprocessing(features)\n",
    "    features = load_features_data()\n",
    "\n",
    "    \n",
    "    model = train_model(features)\n",
    "    model.save_model('lgb_classifier.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f8bc8773-c3ad-46df-b64e-d06e85bba07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2654, number of negative: 372894\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 942\n",
      "[LightGBM] [Info] Number of data points in the train set: 375548, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007067 -> initscore=-4.945226\n",
      "[LightGBM] [Info] Start training from score -4.945226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     93250\n",
      "           1       0.51      0.20      0.29       637\n",
      "\n",
      "    accuracy                           0.99     93887\n",
      "   macro avg       0.75      0.60      0.64     93887\n",
      "weighted avg       0.99      0.99      0.99     93887\n",
      "\n",
      "Wirh accuracy: 0.9932685036267002\n"
     ]
    }
   ],
   "source": [
    "model = train_model(load_features_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962b1d9-5c2f-4b8c-ac09-150f027a53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "187492eb-d525-4bcb-8d2e-647c597aef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "\n",
    "# # Загрузка данных\n",
    "# def load_features_data():\n",
    "#     features = pd.read_parquet('user_features.pq')\n",
    "#     return features\n",
    "\n",
    "# # Функция для обучения модели\n",
    "# def train_model(X_train, X_test, y_train, y_test, trial):\n",
    "#     # Определение пространства гиперпараметров для оптимизации\n",
    "#     params = {\n",
    "#         'objective': 'binary',\n",
    "#         'metric': 'binary_logloss',\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Логарифмическая сетка\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 12),  # Ограничение по глубине дерева\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#     }\n",
    "    \n",
    "#     # Создание данных для обучения и валидации\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train)\n",
    "#     valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "#     # Обучение модели\n",
    "#     model = lgb.train(params, train_data, valid_sets=[valid_data], num_boost_round=100)\n",
    "    \n",
    "#     # Предсказания и метрики\n",
    "#     y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "#     return precision, recall\n",
    "\n",
    "# # Функция оптимизации\n",
    "# def objective(trial):\n",
    "#     features = load_features_data()\n",
    "    \n",
    "#     # Целевая переменная: наличие заказов\n",
    "#     features = features.drop('user_id', axis=1)\n",
    "#     X = features.drop(columns=['n_orders'])  # Признаки\n",
    "#     y = (features['n_orders'] > 0).astype(int)  # Купил ли товар\n",
    "    \n",
    "#     # Разделение на тренировочные и тестовые данные\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Обучение модели с текущими гиперпараметрами\n",
    "#     precision, recall = train_model(X_train, X_test, y_train, y_test, trial)\n",
    "    \n",
    "#     # Оптимизируем по F1-мере (или precision/recall, в зависимости от задачи)\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "#     return f1_score\n",
    "\n",
    "# # Оптимизация гиперпараметров с использованием Optuna\n",
    "# def optimize_params():\n",
    "#     # Создание объекта оптимизации\n",
    "#     study = optuna.create_study(direction='maximize')  # Мы максимизируем F1-меру\n",
    "#     study.optimize(objective, n_trials=1000)  # Оптимизируем на 50 итерациях (можно увеличить для лучшего результата)\n",
    "    \n",
    "#     print(f\"Best trial: {study.best_trial.params}\")\n",
    "#     return study.best_trial.params\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Поиск оптимальных гиперпараметров\n",
    "#     best_params = optimize_params()\n",
    "    \n",
    "#     # Загрузка данных с уже найденными оптимальными параметрами\n",
    "#     features = load_features_data()\n",
    "    \n",
    "#     # Разделение на признаки и целевую переменную\n",
    "#     features = features.drop('user_id', axis=1)\n",
    "#     X = features.drop(columns=['n_orders'])\n",
    "#     y = (features['n_orders'] > 0).astype(int)\n",
    "    \n",
    "#     # Разделение на тренировочные и тестовые данные\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Использование оптимальных параметров для обучения модели\n",
    "#     model = lgb.train(best_params, lgb.Dataset(X_train, label=y_train), valid_sets=[lgb.Dataset(X_test, label=y_test)], num_boost_round=100)\n",
    "    \n",
    "#     # Предсказания и вывод метрик\n",
    "#     y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    \n",
    "#     # Сохранение модели\n",
    "#     model.save_model('optimized_lgb_classifier.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1d613-7229-4c2c-9288-b351b9b72176",
   "metadata": {},
   "source": [
    "# Модель ранжирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194e0f24-9ae6-433f-b13c-cd621a175104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a8b5a76-48e6-4213-a64a-1a9ddca3b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "catalog = pd.read_parquet('stokman_catalog_preprocessed.pq')\n",
    "actions = pd.read_parquet('train_actions.pq')\n",
    "catalog_vector_mapping = pd.read_parquet('catalog_vector_mapping.pq')\n",
    "vectors = np.load('vectors.npz')['arr_0']  # Извлечение эмбеддингов товаров\n",
    "\n",
    "# Преобразование даты в формате datetime\n",
    "actions['date'] = pd.to_datetime(actions['date'])\n",
    "catalog['add_date'] = pd.to_datetime(catalog['add_date'])\n",
    "\n",
    "# Пример слияния данных по product_id\n",
    "catalog = catalog.merge(catalog_vector_mapping, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa83366b-cb9e-4741-b5bd-b443e1999b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6580936/6580936 [01:01<00:00, 107480.60it/s]\n"
     ]
    }
   ],
   "source": [
    "products_counter = {}\n",
    "for i in tqdm(range(0, actions.shape[0])):\n",
    "    prod_array = actions.iat[i, 4]\n",
    "    for i in prod_array:\n",
    "        if i in products_counter:\n",
    "            products_counter[i] += 1\n",
    "        else:\n",
    "            products_counter[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec8e06e-edfe-489e-a102-1e98b054c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity(x):\n",
    "    try:\n",
    "        return round(products_counter[x] / actions.shape[0] * 10**5, 2)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c88014-be25-4dd2-af55-8fc4c789dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_generation(catalog_data):\n",
    "    catalog_data['price_diff'] = 1 - catalog_data['price'] / catalog_data['old_price']\n",
    "    catalog_data['popularity'] = catalog_data['product_id'].apply(popularity)\n",
    "    return catalog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ab1715-b003-42ab-bae2-8af4a2d3a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = future_generation(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b6ae23f-f9aa-4969-90d2-22276307fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Развернем массивы product_id в отдельные строки\n",
    "actions = actions.explode('products')\n",
    "\n",
    "# Переименуем колонку для удобства\n",
    "actions = actions.rename(columns={'products': 'product_id'})\n",
    "\n",
    "user_item_interactions = actions.groupby(['user_id', 'product_id']).agg(\n",
    "    views=('action', lambda x: (x == 0).sum()),\n",
    "    likes=('action', lambda x: (x == 1).sum()),\n",
    "    add_to_cart=('action', lambda x: (x == 2).sum()),\n",
    "    orders=('action', lambda x: (x == 5).sum()),\n",
    "    last_action_time=('date', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Считаем общее количество действий пользователя\n",
    "user_features = actions.groupby('user_id').agg(\n",
    "    total_actions=('action', 'count'),\n",
    "    total_orders=('action', lambda x: (x == 5).sum()),\n",
    "    total_views=('action', lambda x: (x == 0).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Признаки товара\n",
    "item_features = actions.groupby('product_id').agg(\n",
    "    total_views=('action', lambda x: (x == 0).sum()),\n",
    "    total_add_to_cart=('action', lambda x: (x == 2).sum()),\n",
    "    total_orders=('action', lambda x: (x == 5).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Объединяем пользовательские и товарные признаки\n",
    "data = user_item_interactions.merge(user_features, on='user_id', how='left')\n",
    "data = data.merge(item_features, on='product_id', how='left')\n",
    "data = data.merge(catalog[['product_id', 'category_id', 'price', 'old_price', 'price_diff', 'popularity']], on='product_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c197704-cd82-4d70-9ee3-aef51832599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17444\\3038899646.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['target'] = (train_data['orders'] > 0).astype(int)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17444\\3038899646.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['target'] = (test_data['orders'] > 0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Предположим, что у нас есть временной промежуток для обучения и тестирования\n",
    "train_end_date = pd.Timestamp('2024-09-21')\n",
    "test_start_date = pd.Timestamp('2024-09-22')\n",
    "\n",
    "# Делаем таргет (покупка или нет) на основе данных о заказах (action == 5)\n",
    "train_data = data[data['last_action_time'] <= train_end_date]\n",
    "train_data['target'] = (train_data['orders'] > 0).astype(int)\n",
    "\n",
    "# Тестовая выборка\n",
    "test_data = data[data['last_action_time'] >= test_start_date]\n",
    "test_data['target'] = (test_data['orders'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f6889a-aced-4a29-a3f0-fefdeb7955b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            self.encoders[col] = LabelEncoder().fit(X[col])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        output = X.copy()\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            output[col] = self.encoders[col].transform(X[col])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        output = X.copy()\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            output[col] = self.encoders[col].inverse_transform(X[col])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d6af794-7cc3-4a1d-80f1-2b95da03c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['user_id']\n",
    "\n",
    "train_encoder = MultiColumnLabelEncoder(columns_to_encode)\n",
    "train_data = train_encoder.fit_transform(train_data)\n",
    "\n",
    "test_encoder = MultiColumnLabelEncoder(columns_to_encode)\n",
    "test_data = test_encoder.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c16dcc6-062c-408d-a5a7-8520af6f25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Присоединяем эмбеддинги товаров\n",
    "catalog_vector_mapping = catalog_vector_mapping.set_index('vector_id')\n",
    "vectors = pd.DataFrame(vectors)\n",
    "catalog_mapping = catalog_vector_mapping.merge(vectors, left_index=True, right_index=True)\n",
    "\n",
    "train_data = train_data.merge(catalog_mapping, on='product_id', how='left')\n",
    "test_data = test_data.merge(catalog_mapping, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee4eb34c-bdf1-49ba-b2c0-07ffc5587e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.405917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 100288\n",
      "[LightGBM] [Info] Number of data points in the train set: 370637, number of used features: 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 12:13:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'ranked_model'.\n",
      "2024/10/12 12:13:24 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ranked_model, version 1\n",
      "Created version '1' of model 'ranked_model'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.9919\n",
      "Validation Recall: 0.9876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79468/79468 [00:01<00:00, 53291.15it/s]\n",
      "2024/10/12 12:13:28 INFO mlflow.tracking._tracking_service.client: 🏃 View run crawling-roo-883 at: http://127.0.0.1:5000/#/experiments/0/runs/3aa8efe27e0644e3baa85ff0984379c3.\n",
      "2024/10/12 12:13:28 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Recall: 0.9979\n"
     ]
    }
   ],
   "source": [
    "# Подготовка данных для обучения\n",
    "train_data = pd.read_parquet('train_data.pq')\n",
    "test_data = pd.read_parquet('test_data.pq')\n",
    "\n",
    "train_data = train_data.fillna(0)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "train_data['category_id'] = train_data['category_id'].astype(int) \n",
    "test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "\n",
    "# Подготовим данные для обучения\n",
    "X = train_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])  # Удаляем также 'orders' и 'target'\n",
    "y = train_data['target']  # Используем целевую переменную\n",
    "\n",
    "# Получаем список пользователей\n",
    "groups = train_data['user_id']\n",
    "\n",
    "# Создаем объект GroupShuffleSplit для корректного разбиения данных по пользователям\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Разделяем данные на тренировочный и валидационный наборы, гарантируя, что данные одного пользователя будут в одном наборе\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "# Подготовим тренировочные и валидационные данные\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "# Считаем размер групп для каждого набора\n",
    "train_groups = X_train.groupby('user_id').size().values\n",
    "val_groups = X_val.groupby('user_id').size().values\n",
    "\n",
    "# Создаем обучающие и валидационные наборы для LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)\n",
    "\n",
    "# Параметры для модели LambdaRank\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Параметры модели\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'learning_rate': 0.07,\n",
    "    'num_leaves': 49,\n",
    "    'min_data_in_leaf': 84,\n",
    "    'feature_fraction': 0.95,\n",
    "    'bagging_fraction': 0.93,\n",
    "    'bagging_freq': 3,\n",
    "    'lambda_l1': 0.0129,\n",
    "    'lambda_l2': 0.000137\n",
    "}\n",
    "\n",
    "# Начало трекинга\n",
    "with mlflow.start_run():\n",
    "    # Логирование параметров\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Обучение модели\n",
    "    ranker = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid'],\n",
    "    )\n",
    "\n",
    "    # Логирование модели\n",
    "    mlflow.lightgbm.log_model(ranker, artifact_path=\"lgbm_ranking_model\", registered_model_name='ranked_model')\n",
    "\n",
    "    # Предсказания на валидационных данных\n",
    "    y_val_pred = ranker.predict(X_val)\n",
    "\n",
    "    # Преобразуем предсказания в бинарные (используем порог 0.5)\n",
    "    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "    # Расчет метрик precision и recall на валидационных данных\n",
    "    precision_val = precision_score(y_val, y_val_pred_binary, average='weighted')\n",
    "    recall_val = recall_score(y_val, y_val_pred_binary, average='weighted')\n",
    "\n",
    "    # Логирование precision и recall на валидационных данных в MLflow\n",
    "    mlflow.log_metric(\"precision_val\", precision_val)\n",
    "    mlflow.log_metric(\"recall_val\", recall_val)\n",
    "\n",
    "    print(f'Validation Precision: {precision_val:.4f}')\n",
    "    print(f'Validation Recall: {recall_val:.4f}')\n",
    "\n",
    "    # Подготовка тестовых данных\n",
    "    X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "    test_data['pred'] = ranker.predict(X_test)\n",
    "\n",
    "    # Ранжирование товаров для каждого пользователя\n",
    "    test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "    top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "    # Группировка и сохранение результатов\n",
    "    result = top_25.groupby('user_id').agg({\n",
    "        'product_id': lambda x: list(x)\n",
    "    }).reset_index()\n",
    "\n",
    "    purchased_data = test_data[test_data['orders'] > 0]\n",
    "    purchased_products = purchased_data.groupby('user_id').agg({\n",
    "        'product_id': lambda x: list(x)\n",
    "    }).reset_index()\n",
    "\n",
    "    table = result.merge(purchased_products, on='user_id', how='left')\n",
    "    table = table.fillna(0)\n",
    "\n",
    "    # Функция для расчета recall\n",
    "    def recall(table):\n",
    "        recall_values = []\n",
    "        for i in tqdm(range(0, table.shape[0])):\n",
    "            preds = table.iat[i, 1]\n",
    "            buys = table.iat[i, 2]\n",
    "            if buys == 0:\n",
    "                continue\n",
    "            intersect = list(set(preds) & set(buys))\n",
    "            recall_values.append(len(intersect) / len(buys))\n",
    "\n",
    "        return np.mean(recall_values)\n",
    "\n",
    "    # Расчет метрики recall на тестовых данных\n",
    "    recall_test = recall(table)\n",
    "\n",
    "    # Логирование метрики recall на тестовых данных в MLflow\n",
    "    mlflow.log_metric(\"recall_test\", recall_test)\n",
    "\n",
    "    print(f'Test Recall: {recall_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25be58-5f95-4098-88c9-4287f8973465",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MLFlowTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "695c3488-0101-4228-980e-52e3b8fe6636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1, Stage: None, Status: READY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17444\\4123151431.py:10: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  model_versions = client.get_latest_versions(model_name)\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Инициализируем клиента MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Название вашей модели (замените 'ranked_model' на нужное имя)\n",
    "model_name = \"ranked_model\"\n",
    "\n",
    "# Получаем информацию обо всех версиях модели\n",
    "model_versions = client.get_latest_versions(model_name)\n",
    "\n",
    "# Выводим информацию о стадиях каждой версии модели\n",
    "for version in model_versions:\n",
    "    print(f\"Version: {version.version}, Stage: {version.current_stage}, Status: {version.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eede1332-a07b-4a92-8958-12287b5865bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель ranked_model версии 1 переведена в стадию Production\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17444\\375088842.py:16: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Инициализируем клиента MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Название вашей модели\n",
    "model_name = \"ranked_model\"\n",
    "\n",
    "# Версия модели, которую нужно перевести в новую стадию (укажите версию вашей модели)\n",
    "model_version = 1  # Замените на актуальную версию\n",
    "\n",
    "# Новая стадия (например, \"Staging\" или \"Production\")\n",
    "new_stage = \"Production\"  # Может быть \"Staging\", \"Production\", или \"Archived\"\n",
    "\n",
    "# Переводим модель в новую стадию\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=new_stage\n",
    ")\n",
    "\n",
    "print(f\"Модель {model_name} версии {model_version} переведена в стадию {new_stage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "250234dc-946d-4bef-9709-65f7c77ea925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\anaconda3\\Lib\\site-packages\\mlflow\\store\\artifact\\utils\\models.py:31: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest = client.get_latest_versions(name, None if stage is None else [stage])\n",
      "100%|██████████| 79468/79468 [00:01<00:00, 56864.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Recall: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "import pandas as pd\n",
    "\n",
    "# URI модели из MLflow Model Registry (например, \"models:/<model_name>/Production\")\n",
    "model_uri = \"models:/ranked_model/Production\"  # Замените на своё имя модели\n",
    "\n",
    "# Загрузка модели из MLflow\n",
    "model = mlflow.lightgbm.load_model(model_uri)\n",
    "\n",
    "# Подготовка тестового набора данных\n",
    "test_data = pd.read_parquet('test_data.pq')\n",
    "test_data = test_data.fillna(0)\n",
    "test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "\n",
    "# Использование загруженной модели для предсказания на тестовых данных\n",
    "test_data['pred'] = model.predict(X_test)\n",
    "\n",
    "# Ранжирование товаров для каждого пользователя по предсказанным оценкам\n",
    "test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "\n",
    "# Выбираем топ-25 товаров для каждого пользователя\n",
    "top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "# Преобразуем товары в массив для каждого пользователя\n",
    "result = top_25.groupby('user_id').agg({\n",
    "    'product_id': lambda x: list(x),  # Преобразуем товары в список\n",
    "    'pred': lambda x: list(x)  # Преобразуем предсказанные оценки в список (опционально)\n",
    "}).reset_index()\n",
    "\n",
    "# Фильтрация только купленных товаров (предполагается, что 'orders' > 0 означает покупку)\n",
    "purchased_data = test_data[test_data['orders'] > 0]\n",
    "\n",
    "# Группировка по user_id и преобразование списка купленных товаров в массив\n",
    "purchased_products = purchased_data.groupby('user_id').agg({\n",
    "    'product_id': lambda x: list(x)  # Преобразуем купленные товары в список\n",
    "}).reset_index()\n",
    "\n",
    "# Выводим результат\n",
    "purchased_products.columns = ['user_id', 'bought_products']\n",
    "\n",
    "# Объединение результатов\n",
    "table = result.merge(purchased_products, on='user_id', how='left')\n",
    "table = table.fillna(0).drop('pred', axis=1)\n",
    "\n",
    "# Функция для расчета recall\n",
    "def recall(table):\n",
    "    recall_values = []\n",
    "    for i in tqdm(range(0, table.shape[0])):\n",
    "        preds = table.iat[i, 1]\n",
    "        buys = table.iat[i, 2]\n",
    "        if buys == 0:\n",
    "            continue\n",
    "        intersect = list(set(preds) & set(buys))\n",
    "        recall_values.append(len(intersect) / len(buys))\n",
    "\n",
    "    return np.mean(recall_values)\n",
    "\n",
    "# Расчет recall на тестовых данных\n",
    "recall_test = recall(table)\n",
    "\n",
    "print(f'Test Recall: {recall_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deda542c-1fc4-49b8-8f8b-af57aea52c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9918\n",
      "Recall: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# # Предсказания на валидационных данных\n",
    "# y_val_pred = ranker.predict(X_val)\n",
    "\n",
    "# # Преобразуем предсказания в бинарный формат\n",
    "# y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "# # Расчет метрик точности\n",
    "# # Поскольку у нас многоклассовый случай, выбираем 'weighted'\n",
    "# precision = precision_score(y_val, y_val_pred_binary, average='weighted')\n",
    "# recall = recall_score(y_val, y_val_pred_binary, average='weighted')\n",
    "\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76766734-d55c-44bc-8f5a-2e31005e2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Подготовка тестового набора\n",
    "# X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "\n",
    "# # Предсказания для тестового набора\n",
    "# test_data['pred'] = ranker.predict(X_test)\n",
    "\n",
    "# # Ранжируем товары для каждого пользователя по предсказанным оценкам\n",
    "# test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "\n",
    "# # Выбираем топ-25 товаров для каждого пользователя\n",
    "# top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "# # Преобразуем товары в массив для каждого пользователя\n",
    "# result = top_25.groupby('user_id').agg({\n",
    "#     'product_id': lambda x: list(x),  # Преобразуем товары в список\n",
    "#     'pred': lambda x: list(x)  # Преобразуем предсказанные оценки в список (опционально)\n",
    "# }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a1e265c-11b7-4e2f-8e25-615c892d1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Фильтрация только купленных товаров (предполагается, что 'orders' > 0 означает покупку)\n",
    "# purchased_data = test_data[test_data['orders'] > 0]\n",
    "\n",
    "# # Группировка по user_id и преобразование списка купленных товаров в массив\n",
    "# purchased_products = purchased_data.groupby('user_id').agg({\n",
    "#     'product_id': lambda x: list(x)  # Преобразуем купленные товары в список\n",
    "# }).reset_index()\n",
    "\n",
    "# # Выводим результат\n",
    "# purchased_products.columns = ['user_id', 'bought_products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64051821-bac4-48ac-a7d4-8a9681c4f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = result.merge(purchased_products, on='user_id', how='left')\n",
    "# table = table.fillna(0).drop('pred', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebd3f441-f16b-462a-872b-5577fee76287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recall(table):\n",
    "#     recall = []\n",
    "#     for i in tqdm(range(0, table.shape[0])):\n",
    "#         preds = table.iat[i, 1]\n",
    "#         buys = table.iat[i, 2]\n",
    "#         if buys == 0:\n",
    "#             pass\n",
    "#         else:\n",
    "#             intersect = list(set(preds) & set(buys))\n",
    "#             recall.append(len(intersect) / len(buys))\n",
    "    \n",
    "    \n",
    "#     return np.mean(recall)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd7873ee-b2e0-4da5-81c7-9811f6103fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79468/79468 [00:01<00:00, 54294.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# score = recall(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5229eb85-8c35-452b-afc3-ac467416caae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979641708474539"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3698c52-1a65-4142-b121-13f949f54e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import GroupShuffleSplit\n",
    "# from sklearn.metrics import ndcg_score\n",
    "\n",
    "# # Функция для обучения и оценки модели\n",
    "# def objective(trial):\n",
    "#     # Гиперпараметры, которые мы будем оптимизировать\n",
    "#     params = {\n",
    "#         'objective': 'lambdarank',\n",
    "#         'metric': 'ndcg',\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True), \n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "#         'lambda_l1': trial.suggest_float('lambda_l1', 1e-4, 1.0, log=True),\n",
    "#         'lambda_l2': trial.suggest_float('lambda_l2', 1e-4, 1.0, log=True)\n",
    "#     }\n",
    "\n",
    "#     # Разделяем данные на тренировочные и валидационные\n",
    "#     gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#     train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "#     X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "#     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "#     train_groups = X_train.groupby('user_id').size().values\n",
    "#     val_groups = X_val.groupby('user_id').size().values\n",
    "\n",
    "#     # Создаем обучающие и валидационные наборы для LightGBM\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "#     val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)\n",
    "\n",
    "#     # Обучение модели\n",
    "#     ranker = lgb.train(\n",
    "#         params,\n",
    "#         train_data,\n",
    "#         num_boost_round=100,\n",
    "#         valid_sets=[train_data, val_data],\n",
    "#         valid_names=['train', 'valid'],\n",
    "#     )\n",
    "    \n",
    "#     # Предсказания для тестового набора\n",
    "#     test_data['pred'] = ranker.predict(X_test)\n",
    "    \n",
    "#     # Ранжируем товары для каждого пользователя по предсказанным оценкам\n",
    "#     test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "    \n",
    "#     # Выбираем топ-25 товаров для каждого пользователя\n",
    "#     top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "    \n",
    "#     # Преобразуем товары в массив для каждого пользователя\n",
    "#     result = top_25.groupby('user_id').agg({\n",
    "#         'product_id': lambda x: list(x),  # Преобразуем товары в список\n",
    "#         'pred': lambda x: list(x)  # Преобразуем предсказанные оценки в список (опционально)\n",
    "#     }).reset_index()\n",
    "\n",
    "#     table = result.merge(purchased_products, on='user_id', how='left')\n",
    "#     table = table.fillna(0).drop('pred', axis=1)\n",
    "\n",
    "#     return recall(table)\n",
    "\n",
    "    \n",
    "\n",
    "# # Создание объекта исследования Optuna\n",
    "# study = optuna.create_study(direction='maximize')  # Максимизируем NDCG\n",
    "# study.optimize(objective, n_trials=50)  # Запускаем 50 испытаний\n",
    "\n",
    "# # Выводим результаты\n",
    "# print(\"Best hyperparameters: \", study.best_params)\n",
    "# print(\"Best Recall: \", study.best_value)\n",
    "\n",
    "# ##В качестве метрики сделать массив со всеми купленными пользователем товарами, так же седалть массив с топ 25 товарами по предсказанию \n",
    "# ##По этим двум столбцам считать recall типо сколько купленных товаров в нашем предсказанном массиве и делаить на общее число покупок"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944674b3-8c8f-4acd-b171-3d95c007de3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7871087d-ddc5-499c-a16b-baae21a464ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import timedelta\n",
    "\n",
    "# Преобразование даты и извлечение признаков\n",
    "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Ничего не нужно обучать\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Преобразование даты\n",
    "        X['date'] = pd.to_datetime(X['date'])\n",
    "\n",
    "        # Присоединение каталога товаров\n",
    "        X = X.explode('products')  # Распаковка массива products\n",
    "        X = X.rename(columns={'products': 'product_id'})\n",
    "        # Пример присоединения других данных, пока используем фиктивные данные\n",
    "        X['price'] = np.random.rand(X.shape[0])  # Фиктивные данные\n",
    "        X['category_id'] = np.random.randint(1, 10, size=X.shape[0])  # Фиктивные данные\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "# Генерация признаков\n",
    "class FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Ничего не нужно обучать\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Время с последнего действия для каждого пользователя\n",
    "        X['days_since_last_action'] = X.groupby('user_id')['date'].diff().dt.days\n",
    "\n",
    "        # Признаки активности за последние 3 дня\n",
    "        def count_recent_actions(df, days):\n",
    "            recent = df[df['date'] >= df['date'].max() - pd.Timedelta(days=days)]\n",
    "            return recent.groupby('user_id')['action'].count()\n",
    "\n",
    "        recent_activity_3d = count_recent_actions(X, 3)\n",
    "\n",
    "        # Количество покупок, добавлений в корзину и просмотров\n",
    "        agg_features = X.groupby('user_id').agg({\n",
    "            'product_id': 'nunique',  # Количество уникальных товаров\n",
    "            'price': 'mean',          # Средняя цена товаров\n",
    "            'action': ['count', lambda x: (x == 5).sum()],  # Количество действий и покупок\n",
    "            'days_since_last_action': 'min'  # Время с последнего действия\n",
    "        }).reset_index()\n",
    "\n",
    "        agg_features.columns = ['user_id', 'n_unique_products', 'avg_price', 'n_actions', 'n_orders', 'days_since_last_action']\n",
    "\n",
    "        # Объединение с активностью за 3 дня\n",
    "        agg_features = agg_features.merge(recent_activity_3d, on='user_id', how='left')\n",
    "        agg_features = agg_features.rename(columns={'action': 'actions_last_3d'})\n",
    "\n",
    "        return agg_features\n",
    "\n",
    "\n",
    "# Преобразователь для работы с отсутствующими значениями\n",
    "class NullDataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Ничего не нужно обучать\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.fillna(0)  # Заполнение пропущенных значений нулями\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "01ab2526-fa00-4cfe-8f8b-ab4b47d087f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[165], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Предсказания\u001b[39;00m\n\u001b[0;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    406\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    409\u001b[0m     cloned_transformer,\n\u001b[0;32m    410\u001b[0m     X,\n\u001b[0;32m    411\u001b[0m     y,\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    414\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    415\u001b[0m     params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    416\u001b[0m )\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1307\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[163], line 16\u001b[0m, in \u001b[0;36mDataPreprocessor.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Преобразование даты\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Присоединение каталога товаров\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mexplode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducts\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Распаковка массива products\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "\n",
    "# Обучение модели LightGBM\n",
    "class LightGBMClassifier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', learning_rate=0.05, num_leaves=31)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "# Создание пайплайна\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', DataPreprocessor()),  # Предобработка данных\n",
    "    ('feature_generator', FeatureGenerator()),  # Генерация признаков\n",
    "    ('null_data', NullDataPreprocessor()),  # Предобработка пропущенных данных\n",
    "    ('classifier', LightGBMClassifier())  # Классификатор LightGBM\n",
    "])\n",
    "\n",
    "# Загрузка данных\n",
    "def load_features_data():\n",
    "    features = pd.read_parquet('user_features.pq')\n",
    "    return features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = load_features_data()\n",
    "\n",
    "    X = features.drop(columns=['n_orders', 'user_id'])  # Признаки\n",
    "    y = (features['n_orders'] > 0).astype(int)  # Целевая переменная\n",
    "\n",
    "    # Разделение на тренировочные и тестовые данные\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Обучение модели\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Оценка качества модели\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
