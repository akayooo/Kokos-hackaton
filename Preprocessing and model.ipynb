{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f28c93-e932-40b2-94df-d6f9dd383f29",
   "metadata": {},
   "source": [
    "# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫—É–ø–∏—Ç / –Ω–µ –∫—É–ø–∏—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ —Ç—Ä–µ—Ö –¥–Ω–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7ba916a-4d76-4c6b-a364-ee4d6a30964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "def load_data():\n",
    "    catalog = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\stokman_catalog_preprocessed.pq')\n",
    "    actions = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\train_actions.pq')\n",
    "    vector_mapping = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\catalog_vector_mapping.pq')\n",
    "    vectors = np.load('C:\\\\Users\\\\nikit\\\\Hackaton\\\\vectors.npz')['arr_0']  # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "\n",
    "    return catalog, actions, vector_mapping, vectors\n",
    "\n",
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "def preprocess_data(actions, catalog, vector_mapping):\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã\n",
    "    actions['date'] = pd.to_datetime(actions['date'])\n",
    "    \n",
    "    # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "    actions = actions.explode('products')  # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –º–∞—Å—Å–∏–≤–∞ products\n",
    "    actions = actions.rename(columns={'products': 'product_id'})\n",
    "    actions = actions.merge(catalog[['product_id', 'price', 'category_id']], on='product_id', how='left')\n",
    "    \n",
    "    # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "    actions = actions.merge(vector_mapping, on='product_id', how='left')\n",
    "    \n",
    "    return actions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    catalog, actions, vector_mapping, vectors = load_data()\n",
    "    actions = preprocess_data(actions, catalog, vector_mapping)\n",
    "    actions.to_parquet('actions_preprocessed.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b84f761-6b98-4911-a490-7287e0f3c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "def generate_features(actions):\n",
    "    # –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    actions['days_since_last_action'] = actions.groupby('user_id')['date'].diff().dt.days\n",
    "    \n",
    "    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è\n",
    "    def count_recent_actions(df, days):\n",
    "        recent = df[df['date'] >= df['date'].max() - pd.Timedelta(days=days)]\n",
    "        return recent.groupby('user_id')['action'].count()\n",
    "    \n",
    "    recent_activity_3d = count_recent_actions(actions, 3)\n",
    "    \n",
    "    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫—É–ø–æ–∫, –¥–æ–±–∞–≤–ª–µ–Ω–∏–π –≤ –∫–æ—Ä–∑–∏–Ω—É –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤\n",
    "    agg_features = actions.groupby('user_id').agg({\n",
    "        'product_id': 'nunique',  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤\n",
    "        'price': 'mean',          # –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "        'action': ['count', lambda x: (x == 5).sum()],  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π –∏ –ø–æ–∫—É–ø–æ–∫\n",
    "        'days_since_last_action': 'min'  # –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è\n",
    "    }).reset_index()\n",
    "    \n",
    "    agg_features.columns = ['user_id', 'n_unique_products', 'avg_price', 'n_actions', 'n_orders', 'days_since_last_action']\n",
    "    \n",
    "    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∑–∞ 3 –¥–Ω—è\n",
    "    agg_features = agg_features.merge(recent_activity_3d, on='user_id', how='left')\n",
    "    agg_features = agg_features.rename(columns={'action': 'actions_last_3d'})\n",
    "    \n",
    "    return agg_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actions = pd.read_parquet('actions_preprocessed.pq')\n",
    "    user_features = generate_features(actions)\n",
    "    \n",
    "    user_features.to_parquet('user_features.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c904f48d-17fd-4b5d-aaa4-180c29d70246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–∫—É–ø–∫–∏:\n",
    "# –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤ - avg_price\n",
    "# –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ - days_since_last_action\n",
    "# –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π - n_actions\n",
    "# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫—É–ø–æ–∫ - n_orders\n",
    "# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ - n_unique_products\n",
    "# –î–µ–π—Å—Ç–≤–∏—è –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ç—Ä–∏ –¥–Ω—è - actions_last_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0cdf3ced-ff30-4fa0-9949-13fd99f91f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "def null_data_preprocessing(user_features):\n",
    "    user_features.fillna(0)\n",
    "    user_features.to_parquet('user_features.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a8c7d77c-e05d-4fce-96dd-3af8d23fae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2654, number of negative: 372894\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 942\n",
      "[LightGBM] [Info] Number of data points in the train set: 375548, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007067 -> initscore=-4.945226\n",
      "[LightGBM] [Info] Start training from score -4.945226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     93250\n",
      "           1       0.51      0.20      0.29       637\n",
      "\n",
      "    accuracy                           0.99     93887\n",
      "   macro avg       0.75      0.60      0.64     93887\n",
      "weighted avg       0.99      0.99      0.99     93887\n",
      "\n",
      "Wirh accuracy: 0.9932685036267002\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "# import mlflow\n",
    "# import mlflow.lightgbm\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "def load_features_data():\n",
    "    features = pd.read_parquet('user_features.pq')\n",
    "    return features\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "def train_model(features):\n",
    "    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –Ω–∞–ª–∏—á–∏–µ –∑–∞–∫–∞–∑–æ–≤\n",
    "    features = features.drop('user_id', axis=1)\n",
    "    \n",
    "    X = features.drop(columns=['n_orders'])  # –ü—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    y = (features['n_orders'] > 0).astype(int)  # –ö—É–ø–∏–ª –ª–∏ —Ç–æ–≤–∞—Ä\n",
    "    \n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "    \n",
    "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "    \n",
    "    # # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ MLFlow\n",
    "    # mlflow.lightgbm.autolog()\n",
    "    \n",
    "    # with mlflow.start_run():\n",
    "    model = lgb.train(params, train_data, valid_sets=[valid_data], num_boost_round=100)\n",
    "    \n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # mlflow.log_metric(\"precision\", precision)\n",
    "    # mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'Wirh accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = load_features_data()\n",
    "    null_data_preprocessing(features)\n",
    "    features = load_features_data()\n",
    "\n",
    "    \n",
    "    model = train_model(features)\n",
    "    model.save_model('lgb_classifier.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f8bc8773-c3ad-46df-b64e-d06e85bba07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2654, number of negative: 372894\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 942\n",
      "[LightGBM] [Info] Number of data points in the train set: 375548, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007067 -> initscore=-4.945226\n",
      "[LightGBM] [Info] Start training from score -4.945226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     93250\n",
      "           1       0.51      0.20      0.29       637\n",
      "\n",
      "    accuracy                           0.99     93887\n",
      "   macro avg       0.75      0.60      0.64     93887\n",
      "weighted avg       0.99      0.99      0.99     93887\n",
      "\n",
      "Wirh accuracy: 0.9932685036267002\n"
     ]
    }
   ],
   "source": [
    "model = train_model(load_features_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962b1d9-5c2f-4b8c-ac09-150f027a53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "187492eb-d525-4bcb-8d2e-647c597aef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "\n",
    "# # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "# def load_features_data():\n",
    "#     features = pd.read_parquet('user_features.pq')\n",
    "#     return features\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "# def train_model(X_train, X_test, y_train, y_test, trial):\n",
    "#     # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
    "#     params = {\n",
    "#         'objective': 'binary',\n",
    "#         'metric': 'binary_logloss',\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —Å–µ—Ç–∫–∞\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 12),  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –≥–ª—É–±–∏–Ω–µ –¥–µ—Ä–µ–≤–∞\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#     }\n",
    "    \n",
    "#     # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train)\n",
    "#     valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "#     # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "#     model = lgb.train(params, train_data, valid_sets=[valid_data], num_boost_round=100)\n",
    "    \n",
    "#     # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏\n",
    "#     y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "#     return precision, recall\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
    "# def objective(trial):\n",
    "#     features = load_features_data()\n",
    "    \n",
    "#     # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –Ω–∞–ª–∏—á–∏–µ –∑–∞–∫–∞–∑–æ–≤\n",
    "#     features = features.drop('user_id', axis=1)\n",
    "#     X = features.drop(columns=['n_orders'])  # –ü—Ä–∏–∑–Ω–∞–∫–∏\n",
    "#     y = (features['n_orders'] > 0).astype(int)  # –ö—É–ø–∏–ª –ª–∏ —Ç–æ–≤–∞—Ä\n",
    "    \n",
    "#     # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ç–µ–∫—É—â–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "#     precision, recall = train_model(X_train, X_test, y_train, y_test, trial)\n",
    "    \n",
    "#     # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–æ F1-–º–µ—Ä–µ (–∏–ª–∏ precision/recall, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏)\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "#     return f1_score\n",
    "\n",
    "# # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Optuna\n",
    "# def optimize_params():\n",
    "#     # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
    "#     study = optuna.create_study(direction='maximize')  # –ú—ã –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º F1-–º–µ—Ä—É\n",
    "#     study.optimize(objective, n_trials=1000)  # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –Ω–∞ 50 –∏—Ç–µ—Ä–∞—Ü–∏—è—Ö (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞)\n",
    "    \n",
    "#     print(f\"Best trial: {study.best_trial.params}\")\n",
    "#     return study.best_trial.params\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "#     best_params = optimize_params()\n",
    "    \n",
    "#     # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "#     features = load_features_data()\n",
    "    \n",
    "#     # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
    "#     features = features.drop('user_id', axis=1)\n",
    "#     X = features.drop(columns=['n_orders'])\n",
    "#     y = (features['n_orders'] > 0).astype(int)\n",
    "    \n",
    "#     # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "#     model = lgb.train(best_params, lgb.Dataset(X_train, label=y_train), valid_sets=[lgb.Dataset(X_test, label=y_test)], num_boost_round=100)\n",
    "    \n",
    "#     # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫\n",
    "#     y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    \n",
    "#     # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "#     model.save_model('optimized_lgb_classifier.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1d613-7229-4c2c-9288-b351b9b72176",
   "metadata": {},
   "source": [
    "# –ú–æ–¥–µ–ª—å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194e0f24-9ae6-433f-b13c-cd621a175104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a8b5a76-48e6-4213-a64a-1a9ddca3b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "catalog = pd.read_parquet('stokman_catalog_preprocessed.pq')\n",
    "actions = pd.read_parquet('train_actions.pq')\n",
    "catalog_vector_mapping = pd.read_parquet('catalog_vector_mapping.pq')\n",
    "vectors = np.load('vectors.npz')['arr_0']  # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ datetime\n",
    "actions['date'] = pd.to_datetime(actions['date'])\n",
    "catalog['add_date'] = pd.to_datetime(catalog['add_date'])\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —Å–ª–∏—è–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ product_id\n",
    "catalog = catalog.merge(catalog_vector_mapping, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa83366b-cb9e-4741-b5bd-b443e1999b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6580936/6580936 [01:01<00:00, 107480.60it/s]\n"
     ]
    }
   ],
   "source": [
    "products_counter = {}\n",
    "for i in tqdm(range(0, actions.shape[0])):\n",
    "    prod_array = actions.iat[i, 4]\n",
    "    for i in prod_array:\n",
    "        if i in products_counter:\n",
    "            products_counter[i] += 1\n",
    "        else:\n",
    "            products_counter[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec8e06e-edfe-489e-a102-1e98b054c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity(x):\n",
    "    try:\n",
    "        return round(products_counter[x] / actions.shape[0] * 10**5, 2)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c88014-be25-4dd2-af55-8fc4c789dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_generation(catalog_data):\n",
    "    catalog_data['price_diff'] = 1 - catalog_data['price'] / catalog_data['old_price']\n",
    "    catalog_data['popularity'] = catalog_data['product_id'].apply(popularity)\n",
    "    return catalog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ab1715-b003-42ab-bae2-8af4a2d3a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = future_generation(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b6ae23f-f9aa-4969-90d2-22276307fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–≤–µ—Ä–Ω–µ–º –º–∞—Å—Å–∏–≤—ã product_id –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
    "actions = actions.explode('products')\n",
    "\n",
    "# –ü–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "actions = actions.rename(columns={'products': 'product_id'})\n",
    "\n",
    "user_item_interactions = actions.groupby(['user_id', 'product_id']).agg(\n",
    "    views=('action', lambda x: (x == 0).sum()),\n",
    "    likes=('action', lambda x: (x == 1).sum()),\n",
    "    add_to_cart=('action', lambda x: (x == 2).sum()),\n",
    "    orders=('action', lambda x: (x == 5).sum()),\n",
    "    last_action_time=('date', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# –°—á–∏—Ç–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "user_features = actions.groupby('user_id').agg(\n",
    "    total_actions=('action', 'count'),\n",
    "    total_orders=('action', lambda x: (x == 5).sum()),\n",
    "    total_views=('action', lambda x: (x == 0).sum())\n",
    ").reset_index()\n",
    "\n",
    "# –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ–≤–∞—Ä–∞\n",
    "item_features = actions.groupby('product_id').agg(\n",
    "    total_views=('action', lambda x: (x == 0).sum()),\n",
    "    total_add_to_cart=('action', lambda x: (x == 2).sum()),\n",
    "    total_orders=('action', lambda x: (x == 5).sum())\n",
    ").reset_index()\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏ —Ç–æ–≤–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "data = user_item_interactions.merge(user_features, on='user_id', how='left')\n",
    "data = data.merge(item_features, on='product_id', how='left')\n",
    "data = data.merge(catalog[['product_id', 'category_id', 'price', 'old_price', 'price_diff', 'popularity']], on='product_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c197704-cd82-4d70-9ee3-aef51832599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17444\\3038899646.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['target'] = (train_data['orders'] > 0).astype(int)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17444\\3038899646.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['target'] = (test_data['orders'] > 0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "train_end_date = pd.Timestamp('2024-09-21')\n",
    "test_start_date = pd.Timestamp('2024-09-22')\n",
    "\n",
    "# –î–µ–ª–∞–µ–º —Ç–∞—Ä–≥–µ—Ç (–ø–æ–∫—É–ø–∫–∞ –∏–ª–∏ –Ω–µ—Ç) –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–∫–∞–∑–∞—Ö (action == 5)\n",
    "train_data = data[data['last_action_time'] <= train_end_date]\n",
    "train_data['target'] = (train_data['orders'] > 0).astype(int)\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞\n",
    "test_data = data[data['last_action_time'] >= test_start_date]\n",
    "test_data['target'] = (test_data['orders'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f6889a-aced-4a29-a3f0-fefdeb7955b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoders = {}\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            self.encoders[col] = LabelEncoder().fit(X[col])\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        output = X.copy()\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            output[col] = self.encoders[col].transform(X[col])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        output = X.copy()\n",
    "        columns = X.columns if self.columns is None else self.columns\n",
    "        for col in columns:\n",
    "            output[col] = self.encoders[col].inverse_transform(X[col])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d6af794-7cc3-4a1d-80f1-2b95da03c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['user_id']\n",
    "\n",
    "train_encoder = MultiColumnLabelEncoder(columns_to_encode)\n",
    "train_data = train_encoder.fit_transform(train_data)\n",
    "\n",
    "test_encoder = MultiColumnLabelEncoder(columns_to_encode)\n",
    "test_data = test_encoder.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c16dcc6-062c-408d-a5a7-8520af6f25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "catalog_vector_mapping = catalog_vector_mapping.set_index('vector_id')\n",
    "vectors = pd.DataFrame(vectors)\n",
    "catalog_mapping = catalog_vector_mapping.merge(vectors, left_index=True, right_index=True)\n",
    "\n",
    "train_data = train_data.merge(catalog_mapping, on='product_id', how='left')\n",
    "test_data = test_data.merge(catalog_mapping, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee4eb34c-bdf1-49ba-b2c0-07ffc5587e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.405917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 100288\n",
      "[LightGBM] [Info] Number of data points in the train set: 370637, number of used features: 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 12:13:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'ranked_model'.\n",
      "2024/10/12 12:13:24 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ranked_model, version 1\n",
      "Created version '1' of model 'ranked_model'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.9919\n",
      "Validation Recall: 0.9876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79468/79468 [00:01<00:00, 53291.15it/s]\n",
      "2024/10/12 12:13:28 INFO mlflow.tracking._tracking_service.client: üèÉ View run crawling-roo-883 at: http://127.0.0.1:5000/#/experiments/0/runs/3aa8efe27e0644e3baa85ff0984379c3.\n",
      "2024/10/12 12:13:28 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Recall: 0.9979\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "train_data = pd.read_parquet('train_data.pq')\n",
    "test_data = pd.read_parquet('test_data.pq')\n",
    "\n",
    "train_data = train_data.fillna(0)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "train_data['category_id'] = train_data['category_id'].astype(int) \n",
    "test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "X = train_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])  # –£–¥–∞–ª—è–µ–º —Ç–∞–∫–∂–µ 'orders' –∏ 'target'\n",
    "y = train_data['target']  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "groups = train_data['user_id']\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç GroupShuffleSplit –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä—ã, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –±—É–¥—É—Ç –≤ –æ–¥–Ω–æ–º –Ω–∞–±–æ—Ä–µ\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "# –°—á–∏—Ç–∞–µ–º —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "train_groups = X_train.groupby('user_id').size().values\n",
    "val_groups = X_val.groupby('user_id').size().values\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–ª—è LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏ LambdaRank\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'learning_rate': 0.07,\n",
    "    'num_leaves': 49,\n",
    "    'min_data_in_leaf': 84,\n",
    "    'feature_fraction': 0.95,\n",
    "    'bagging_fraction': 0.93,\n",
    "    'bagging_freq': 3,\n",
    "    'lambda_l1': 0.0129,\n",
    "    'lambda_l2': 0.000137\n",
    "}\n",
    "\n",
    "# –ù–∞—á–∞–ª–æ —Ç—Ä–µ–∫–∏–Ω–≥–∞\n",
    "with mlflow.start_run():\n",
    "    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    ranker = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid'],\n",
    "    )\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    mlflow.lightgbm.log_model(ranker, artifact_path=\"lgbm_ranking_model\", registered_model_name='ranked_model')\n",
    "\n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    y_val_pred = ranker.predict(X_val)\n",
    "\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 0.5)\n",
    "    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ precision –∏ recall –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    precision_val = precision_score(y_val, y_val_pred_binary, average='weighted')\n",
    "    recall_val = recall_score(y_val, y_val_pred_binary, average='weighted')\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ precision –∏ recall –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ MLflow\n",
    "    mlflow.log_metric(\"precision_val\", precision_val)\n",
    "    mlflow.log_metric(\"recall_val\", recall_val)\n",
    "\n",
    "    print(f'Validation Precision: {precision_val:.4f}')\n",
    "    print(f'Validation Recall: {recall_val:.4f}')\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "    test_data['pred'] = ranker.predict(X_test)\n",
    "\n",
    "    # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "    top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    result = top_25.groupby('user_id').agg({\n",
    "        'product_id': lambda x: list(x)\n",
    "    }).reset_index()\n",
    "\n",
    "    purchased_data = test_data[test_data['orders'] > 0]\n",
    "    purchased_products = purchased_data.groupby('user_id').agg({\n",
    "        'product_id': lambda x: list(x)\n",
    "    }).reset_index()\n",
    "\n",
    "    table = result.merge(purchased_products, on='user_id', how='left')\n",
    "    table = table.fillna(0)\n",
    "\n",
    "    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ recall\n",
    "    def recall(table):\n",
    "        recall_values = []\n",
    "        for i in tqdm(range(0, table.shape[0])):\n",
    "            preds = table.iat[i, 1]\n",
    "            buys = table.iat[i, 2]\n",
    "            if buys == 0:\n",
    "                continue\n",
    "            intersect = list(set(preds) & set(buys))\n",
    "            recall_values.append(len(intersect) / len(buys))\n",
    "\n",
    "        return np.mean(recall_values)\n",
    "\n",
    "    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ recall –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    recall_test = recall(table)\n",
    "\n",
    "    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ recall –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ MLflow\n",
    "    mlflow.log_metric(\"recall_test\", recall_test)\n",
    "\n",
    "    print(f'Test Recall: {recall_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25be58-5f95-4098-88c9-4287f8973465",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MLFlowTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "695c3488-0101-4228-980e-52e3b8fe6636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1, Stage: None, Status: READY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17444\\4123151431.py:10: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  model_versions = client.get_latest_versions(model_name)\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç–∞ MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# –ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ 'ranked_model' –Ω–∞ –Ω—É–∂–Ω–æ–µ –∏–º—è)\n",
    "model_name = \"ranked_model\"\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–±–æ –≤—Å–µ—Ö –≤–µ—Ä—Å–∏—è—Ö –º–æ–¥–µ–ª–∏\n",
    "model_versions = client.get_latest_versions(model_name)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞–¥–∏—è—Ö –∫–∞–∂–¥–æ–π –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏\n",
    "for version in model_versions:\n",
    "    print(f\"Version: {version.version}, Stage: {version.current_stage}, Status: {version.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eede1332-a07b-4a92-8958-12287b5865bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–¥–µ–ª—å ranked_model –≤–µ—Ä—Å–∏–∏ 1 –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ –≤ —Å—Ç–∞–¥–∏—é Production\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17444\\375088842.py:16: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç–∞ MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# –ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "model_name = \"ranked_model\"\n",
    "\n",
    "# –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –≤ –Ω–æ–≤—É—é —Å—Ç–∞–¥–∏—é (—É–∫–∞–∂–∏—Ç–µ –≤–µ—Ä—Å–∏—é –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏)\n",
    "model_version = 1  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é\n",
    "\n",
    "# –ù–æ–≤–∞—è —Å—Ç–∞–¥–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"Staging\" –∏–ª–∏ \"Production\")\n",
    "new_stage = \"Production\"  # –ú–æ–∂–µ—Ç –±—ã—Ç—å \"Staging\", \"Production\", –∏–ª–∏ \"Archived\"\n",
    "\n",
    "# –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ –Ω–æ–≤—É—é —Å—Ç–∞–¥–∏—é\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=new_stage\n",
    ")\n",
    "\n",
    "print(f\"–ú–æ–¥–µ–ª—å {model_name} –≤–µ—Ä—Å–∏–∏ {model_version} –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ –≤ —Å—Ç–∞–¥–∏—é {new_stage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "250234dc-946d-4bef-9709-65f7c77ea925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\anaconda3\\Lib\\site-packages\\mlflow\\store\\artifact\\utils\\models.py:31: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest = client.get_latest_versions(name, None if stage is None else [stage])\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79468/79468 [00:01<00:00, 56864.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Recall: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "import pandas as pd\n",
    "\n",
    "# URI –º–æ–¥–µ–ª–∏ –∏–∑ MLflow Model Registry (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"models:/<model_name>/Production\")\n",
    "model_uri = \"models:/ranked_model/Production\"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ—ë –∏–º—è –º–æ–¥–µ–ª–∏\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ MLflow\n",
    "model = mlflow.lightgbm.load_model(model_uri)\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "test_data = pd.read_parquet('test_data.pq')\n",
    "test_data = test_data.fillna(0)\n",
    "test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "test_data['pred'] = model.predict(X_test)\n",
    "\n",
    "# –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –æ—Ü–µ–Ω–∫–∞–º\n",
    "test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "\n",
    "# –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-25 —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–≤–∞—Ä—ã –≤ –º–∞—Å—Å–∏–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "result = top_25.groupby('user_id').agg({\n",
    "    'product_id': lambda x: list(x),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–≤–∞—Ä—ã –≤ —Å–ø–∏—Å–æ–∫\n",
    "    'pred': lambda x: list(x)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤ —Å–ø–∏—Å–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "}).reset_index()\n",
    "\n",
    "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –∫—É–ø–ª–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ 'orders' > 0 –æ–∑–Ω–∞—á–∞–µ—Ç –ø–æ–∫—É–ø–∫—É)\n",
    "purchased_data = test_data[test_data['orders'] > 0]\n",
    "\n",
    "# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ user_id –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫—É–ø–ª–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –º–∞—Å—Å–∏–≤\n",
    "purchased_products = purchased_data.groupby('user_id').agg({\n",
    "    'product_id': lambda x: list(x)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫—É–ø–ª–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –≤ —Å–ø–∏—Å–æ–∫\n",
    "}).reset_index()\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "purchased_products.columns = ['user_id', 'bought_products']\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "table = result.merge(purchased_products, on='user_id', how='left')\n",
    "table = table.fillna(0).drop('pred', axis=1)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ recall\n",
    "def recall(table):\n",
    "    recall_values = []\n",
    "    for i in tqdm(range(0, table.shape[0])):\n",
    "        preds = table.iat[i, 1]\n",
    "        buys = table.iat[i, 2]\n",
    "        if buys == 0:\n",
    "            continue\n",
    "        intersect = list(set(preds) & set(buys))\n",
    "        recall_values.append(len(intersect) / len(buys))\n",
    "\n",
    "    return np.mean(recall_values)\n",
    "\n",
    "# –†–∞—Å—á–µ—Ç recall –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "recall_test = recall(table)\n",
    "\n",
    "print(f'Test Recall: {recall_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deda542c-1fc4-49b8-8f8b-af57aea52c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9918\n",
      "Recall: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "# y_val_pred = ranker.predict(X_val)\n",
    "\n",
    "# # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç\n",
    "# y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "# # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "# # –ü–æ—Å–∫–æ–ª—å–∫—É —É –Ω–∞—Å –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π —Å–ª—É—á–∞–π, –≤—ã–±–∏—Ä–∞–µ–º 'weighted'\n",
    "# precision = precision_score(y_val, y_val_pred_binary, average='weighted')\n",
    "# recall = recall_score(y_val, y_val_pred_binary, average='weighted')\n",
    "\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76766734-d55c-44bc-8f5a-2e31005e2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "# X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "\n",
    "# # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "# test_data['pred'] = ranker.predict(X_test)\n",
    "\n",
    "# # –†–∞–Ω–∂–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –æ—Ü–µ–Ω–∫–∞–º\n",
    "# test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "\n",
    "# # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-25 —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "# top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "# # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–≤–∞—Ä—ã –≤ –º–∞—Å—Å–∏–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "# result = top_25.groupby('user_id').agg({\n",
    "#     'product_id': lambda x: list(x),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–≤–∞—Ä—ã –≤ —Å–ø–∏—Å–æ–∫\n",
    "#     'pred': lambda x: list(x)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤ —Å–ø–∏—Å–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "# }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a1e265c-11b7-4e2f-8e25-615c892d1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –∫—É–ø–ª–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ 'orders' > 0 –æ–∑–Ω–∞—á–∞–µ—Ç –ø–æ–∫—É–ø–∫—É)\n",
    "# purchased_data = test_data[test_data['orders'] > 0]\n",
    "\n",
    "# # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ user_id –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫—É–ø–ª–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –º–∞—Å—Å–∏–≤\n",
    "# purchased_products = purchased_data.groupby('user_id').agg({\n",
    "#     'product_id': lambda x: list(x)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫—É–ø–ª–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –≤ —Å–ø–∏—Å–æ–∫\n",
    "# }).reset_index()\n",
    "\n",
    "# # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "# purchased_products.columns = ['user_id', 'bought_products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64051821-bac4-48ac-a7d4-8a9681c4f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = result.merge(purchased_products, on='user_id', how='left')\n",
    "# table = table.fillna(0).drop('pred', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebd3f441-f16b-462a-872b-5577fee76287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recall(table):\n",
    "#     recall = []\n",
    "#     for i in tqdm(range(0, table.shape[0])):\n",
    "#         preds = table.iat[i, 1]\n",
    "#         buys = table.iat[i, 2]\n",
    "#         if buys == 0:\n",
    "#             pass\n",
    "#         else:\n",
    "#             intersect = list(set(preds) & set(buys))\n",
    "#             recall.append(len(intersect) / len(buys))\n",
    "    \n",
    "    \n",
    "#     return np.mean(recall)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd7873ee-b2e0-4da5-81c7-9811f6103fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79468/79468 [00:01<00:00, 54294.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# score = recall(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5229eb85-8c35-452b-afc3-ac467416caae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979641708474539"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3698c52-1a65-4142-b121-13f949f54e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import GroupShuffleSplit\n",
    "# from sklearn.metrics import ndcg_score\n",
    "\n",
    "# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏\n",
    "# def objective(trial):\n",
    "#     # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å\n",
    "#     params = {\n",
    "#         'objective': 'lambdarank',\n",
    "#         'metric': 'ndcg',\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True), \n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "#         'lambda_l1': trial.suggest_float('lambda_l1', 1e-4, 1.0, log=True),\n",
    "#         'lambda_l2': trial.suggest_float('lambda_l2', 1e-4, 1.0, log=True)\n",
    "#     }\n",
    "\n",
    "#     # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ\n",
    "#     gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#     train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "#     X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "#     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "#     train_groups = X_train.groupby('user_id').size().values\n",
    "#     val_groups = X_val.groupby('user_id').size().values\n",
    "\n",
    "#     # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–ª—è LightGBM\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "#     val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)\n",
    "\n",
    "#     # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "#     ranker = lgb.train(\n",
    "#         params,\n",
    "#         train_data,\n",
    "#         num_boost_round=100,\n",
    "#         valid_sets=[train_data, val_data],\n",
    "#         valid_names=['train', 'valid'],\n",
    "#     )\n",
    "    \n",
    "#     # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "#     test_data['pred'] = ranker.predict(X_test)\n",
    "    \n",
    "#     # –†–∞–Ω–∂–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –æ—Ü–µ–Ω–∫–∞–º\n",
    "#     test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "    \n",
    "#     # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-25 —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "#     top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "    \n",
    "#     # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–≤–∞—Ä—ã –≤ –º–∞—Å—Å–∏–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "#     result = top_25.groupby('user_id').agg({\n",
    "#         'product_id': lambda x: list(x),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–≤–∞—Ä—ã –≤ —Å–ø–∏—Å–æ–∫\n",
    "#         'pred': lambda x: list(x)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤ —Å–ø–∏—Å–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "#     }).reset_index()\n",
    "\n",
    "#     table = result.merge(purchased_products, on='user_id', how='left')\n",
    "#     table = table.fillna(0).drop('pred', axis=1)\n",
    "\n",
    "#     return recall(table)\n",
    "\n",
    "    \n",
    "\n",
    "# # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è Optuna\n",
    "# study = optuna.create_study(direction='maximize')  # –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º NDCG\n",
    "# study.optimize(objective, n_trials=50)  # –ó–∞–ø—É—Å–∫–∞–µ–º 50 –∏—Å–ø—ã—Ç–∞–Ω–∏–π\n",
    "\n",
    "# # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "# print(\"Best hyperparameters: \", study.best_params)\n",
    "# print(\"Best Recall: \", study.best_value)\n",
    "\n",
    "# ##–í –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–¥–µ–ª–∞—Ç—å –º–∞—Å—Å–∏–≤ —Å–æ –≤—Å–µ–º–∏ –∫—É–ø–ª–µ–Ω–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º —Ç–æ–≤–∞—Ä–∞–º–∏, —Ç–∞–∫ –∂–µ —Å–µ–¥–∞–ª—Ç—å –º–∞—Å—Å–∏–≤ —Å —Ç–æ–ø 25 —Ç–æ–≤–∞—Ä–∞–º–∏ –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é \n",
    "# ##–ü–æ —ç—Ç–∏–º –¥–≤—É–º —Å—Ç–æ–ª–±—Ü–∞–º —Å—á–∏—Ç–∞—Ç—å recall —Ç–∏–ø–æ —Å–∫–æ–ª—å–∫–æ –∫—É–ø–ª–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –Ω–∞—à–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–º –º–∞—Å—Å–∏–≤–µ –∏ –¥–µ–ª–∞–∏—Ç—å –Ω–∞ –æ–±—â–µ–µ —á–∏—Å–ª–æ –ø–æ–∫—É–ø–æ–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944674b3-8c8f-4acd-b171-3d95c007de3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7871087d-ddc5-499c-a16b-baae21a464ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import timedelta\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # –ù–∏—á–µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å\n",
    "\n",
    "    def transform(self, X):\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã\n",
    "        X['date'] = pd.to_datetime(X['date'])\n",
    "\n",
    "        # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "        X = X.explode('products')  # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –º–∞—Å—Å–∏–≤–∞ products\n",
    "        X = X.rename(columns={'products': 'product_id'})\n",
    "        # –ü—Ä–∏–º–µ—Ä –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        X['price'] = np.random.rand(X.shape[0])  # –§–∏–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        X['category_id'] = np.random.randint(1, 10, size=X.shape[0])  # –§–∏–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "class FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # –ù–∏—á–µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å\n",
    "\n",
    "    def transform(self, X):\n",
    "        # –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "        X['days_since_last_action'] = X.groupby('user_id')['date'].diff().dt.days\n",
    "\n",
    "        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è\n",
    "        def count_recent_actions(df, days):\n",
    "            recent = df[df['date'] >= df['date'].max() - pd.Timedelta(days=days)]\n",
    "            return recent.groupby('user_id')['action'].count()\n",
    "\n",
    "        recent_activity_3d = count_recent_actions(X, 3)\n",
    "\n",
    "        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫—É–ø–æ–∫, –¥–æ–±–∞–≤–ª–µ–Ω–∏–π –≤ –∫–æ—Ä–∑–∏–Ω—É –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤\n",
    "        agg_features = X.groupby('user_id').agg({\n",
    "            'product_id': 'nunique',  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤\n",
    "            'price': 'mean',          # –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "            'action': ['count', lambda x: (x == 5).sum()],  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π –∏ –ø–æ–∫—É–ø–æ–∫\n",
    "            'days_since_last_action': 'min'  # –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è\n",
    "        }).reset_index()\n",
    "\n",
    "        agg_features.columns = ['user_id', 'n_unique_products', 'avg_price', 'n_actions', 'n_orders', 'days_since_last_action']\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∑–∞ 3 –¥–Ω—è\n",
    "        agg_features = agg_features.merge(recent_activity_3d, on='user_id', how='left')\n",
    "        agg_features = agg_features.rename(columns={'action': 'actions_last_3d'})\n",
    "\n",
    "        return agg_features\n",
    "\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "class NullDataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # –ù–∏—á–µ–≥–æ –Ω–µ –Ω—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.fillna(0)  # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω—É–ª—è–º–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "01ab2526-fa00-4cfe-8f8b-ab4b47d087f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[165], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\u001b[39;00m\n\u001b[0;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    406\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    409\u001b[0m     cloned_transformer,\n\u001b[0;32m    410\u001b[0m     X,\n\u001b[0;32m    411\u001b[0m     y,\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    414\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    415\u001b[0m     params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    416\u001b[0m )\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1307\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[163], line 16\u001b[0m, in \u001b[0;36mDataPreprocessor.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ç–æ–≤–∞—Ä–æ–≤\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mexplode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducts\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –º–∞—Å—Å–∏–≤–∞ products\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LightGBM\n",
    "class LightGBMClassifier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', learning_rate=0.05, num_leaves=31)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', DataPreprocessor()),  # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    ('feature_generator', FeatureGenerator()),  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    ('null_data', NullDataPreprocessor()),  # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    ('classifier', LightGBMClassifier())  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä LightGBM\n",
    "])\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "def load_features_data():\n",
    "    features = pd.read_parquet('user_features.pq')\n",
    "    return features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = load_features_data()\n",
    "\n",
    "    X = features.drop(columns=['n_orders', 'user_id'])  # –ü—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    y = (features['n_orders'] > 0).astype(int)  # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
