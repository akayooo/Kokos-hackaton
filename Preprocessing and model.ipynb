{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f28c93-e932-40b2-94df-d6f9dd383f29",
   "metadata": {},
   "source": [
    "# Классификатор купит / не купит в течение трех дней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ba916a-4d76-4c6b-a364-ee4d6a30964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Загрузка данных\n",
    "def load_data():\n",
    "    catalog = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\stokman_catalog_preprocessed.pq')\n",
    "    actions = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\train_actions.pq')\n",
    "    vector_mapping = pd.read_parquet('C:\\\\Users\\\\nikit\\\\Hackaton\\\\catalog_vector_mapping.pq')\n",
    "    vectors = np.load('C:\\\\Users\\\\nikit\\\\Hackaton\\\\vectors.npz')['arr_0']  # Извлечение эмбеддингов товаров\n",
    "\n",
    "    return catalog, actions, vector_mapping, vectors\n",
    "\n",
    "# Предобработка данных\n",
    "def preprocess_data(actions, catalog, vector_mapping):\n",
    "    # Преобразование даты\n",
    "    actions['date'] = pd.to_datetime(actions['date'])\n",
    "    \n",
    "    # Присоединение каталога товаров\n",
    "    actions = actions.explode('products')  # Распаковка массива products\n",
    "    actions = actions.rename(columns={'products': 'product_id'})\n",
    "    actions = actions.merge(catalog[['product_id', 'price', 'category_id']], on='product_id', how='left')\n",
    "    \n",
    "    # Присоединение векторов товаров\n",
    "    actions = actions.merge(vector_mapping, on='product_id', how='left')\n",
    "    \n",
    "    return actions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    catalog, actions, vector_mapping, vectors = load_data()\n",
    "    actions = preprocess_data(actions, catalog, vector_mapping)\n",
    "    actions.to_parquet('actions_preprocessed.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b84f761-6b98-4911-a490-7287e0f3c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Генерация временных и взаимодействующих признаков\n",
    "def generate_features(actions):\n",
    "    # Время с последнего действия для каждого пользователя\n",
    "    actions['days_since_last_action'] = actions.groupby('user_id')['date'].diff().dt.days\n",
    "    \n",
    "    # Признаки активности за последние 3 дня\n",
    "    def count_recent_actions(df, days):\n",
    "        recent = df[df['date'] >= df['date'].max() - pd.Timedelta(days=days)]\n",
    "        return recent.groupby('user_id')['action'].count()\n",
    "    \n",
    "    recent_activity_3d = count_recent_actions(actions, 3)\n",
    "    \n",
    "    # Количество покупок, добавлений в корзину и просмотров\n",
    "    agg_features = actions.groupby('user_id').agg({\n",
    "        'product_id': 'nunique',  # Количество уникальных товаров\n",
    "        'price': 'mean',          # Средняя цена товаров\n",
    "        'action': ['count', lambda x: (x == 5).sum()],  # Количество действий и покупок\n",
    "        'days_since_last_action': 'min'  # Время с последнего действия\n",
    "    }).reset_index()\n",
    "    \n",
    "    agg_features.columns = ['user_id', 'n_unique_products', 'avg_price', 'n_actions', 'n_orders', 'days_since_last_action']\n",
    "    \n",
    "    # Объединение с активностью за 3 дня\n",
    "    agg_features = agg_features.merge(recent_activity_3d, on='user_id', how='left')\n",
    "    agg_features = agg_features.rename(columns={'action': 'actions_last_3d'})\n",
    "    \n",
    "    return agg_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actions = pd.read_parquet('actions_preprocessed.pq')\n",
    "    user_features = generate_features(actions)\n",
    "    \n",
    "    user_features.to_parquet('user_features.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c904f48d-17fd-4b5d-aaa4-180c29d70246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сгенерированные фичи для предсказания покупки:\n",
    "# Средняя цена интересующих товаров - avg_price\n",
    "# Время с последней активности - days_since_last_action\n",
    "# Общее количество действий - n_actions\n",
    "# Количество покупок - n_orders\n",
    "# Количество уникальных товаров - n_unique_products\n",
    "# Действия за последние три дня - actions_last_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cdf3ced-ff30-4fa0-9949-13fd99f91f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Предобработка отсутствующих значений\n",
    "def null_data_preprocessing(user_features):\n",
    "    user_features.fillna(0)\n",
    "    user_features.to_parquet('user_features.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c7d77c-e05d-4fce-96dd-3af8d23fae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2654, number of negative: 372894\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 942\n",
      "[LightGBM] [Info] Number of data points in the train set: 375548, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007067 -> initscore=-4.945226\n",
      "[LightGBM] [Info] Start training from score -4.945226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     93250\n",
      "           1       0.51      0.20      0.29       637\n",
      "\n",
      "    accuracy                           0.99     93887\n",
      "   macro avg       0.75      0.60      0.64     93887\n",
      "weighted avg       0.99      0.99      0.99     93887\n",
      "\n",
      "Wirh accuracy: 0.9932685036267002\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "# import mlflow\n",
    "# import mlflow.lightgbm\n",
    "\n",
    "# Загрузка данных\n",
    "def load_features_data():\n",
    "    features = pd.read_parquet('user_features.pq')\n",
    "    return features\n",
    "\n",
    "# Обучение модели\n",
    "def train_model(features):\n",
    "    # Целевая переменная: наличие заказов   \n",
    "    X = features.drop(columns=['n_orders'])  # Признаки\n",
    "    y = (features['n_orders'] > 0).astype(int)  # Купил ли товар\n",
    "    \n",
    "    # Разделение на тренировочные и тестовые данные\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_test_save = X_test.copy(deep=True)\n",
    "    X_train = X_train.drop('user_id', axis=1)\n",
    "    X_test = X_test.drop('user_id', axis=1)\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "    \n",
    "    # Параметры модели\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "    \n",
    "    # # Логирование через MLFlow\n",
    "    # mlflow.lightgbm.autolog()\n",
    "    \n",
    "    # with mlflow.start_run():\n",
    "    model = lgb.train(params, train_data, valid_sets=[valid_data], num_boost_round=100)\n",
    "    \n",
    "    # Предсказания и метрики\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # mlflow.log_metric(\"precision\", precision)\n",
    "    # mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'Wirh accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    \n",
    "    return model, X_test, X_test_save\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = load_features_data()\n",
    "    null_data_preprocessing(features)\n",
    "    features = load_features_data()\n",
    "\n",
    "    \n",
    "    model, X_test, X_test_user = train_model(features)\n",
    "    model.save_model('lgb_classifier.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3962b1d9-5c2f-4b8c-ac09-150f027a53e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2654, number of negative: 372894\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 942\n",
      "[LightGBM] [Info] Number of data points in the train set: 375548, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007067 -> initscore=-4.945226\n",
      "[LightGBM] [Info] Start training from score -4.945226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     93250\n",
      "           1       0.51      0.20      0.29       637\n",
      "\n",
      "    accuracy                           0.99     93887\n",
      "   macro avg       0.75      0.60      0.64     93887\n",
      "weighted avg       0.99      0.99      0.99     93887\n",
      "\n",
      "Wirh accuracy: 0.9932685036267002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50224</th>\n",
       "      <td>a046199a-9f61-11ea-a0d7-002590e45c38</td>\n",
       "      <td>0.893693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81700</th>\n",
       "      <td>4814fc56-f93c-11eb-a6e9-002590c82437</td>\n",
       "      <td>0.886025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74487</th>\n",
       "      <td>81e3ac7e-436d-11ef-86e0-002590c0647c</td>\n",
       "      <td>0.878027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87929</th>\n",
       "      <td>101762d6-7733-11ee-bbb1-002590c82437</td>\n",
       "      <td>0.877128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80924</th>\n",
       "      <td>99f0590a-bea9-11ee-bbb1-002590c82436</td>\n",
       "      <td>0.874505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80705</th>\n",
       "      <td>8f8056b2-2f02-11eb-86e0-002590c0647c</td>\n",
       "      <td>0.053368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62219</th>\n",
       "      <td>e42c4da4-22f0-11ed-a044-002590c82437</td>\n",
       "      <td>0.053361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>890e8a10-9a70-11ee-86e0-002590c0647c</td>\n",
       "      <td>0.053286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14130</th>\n",
       "      <td>cfb1e80e-46a5-11ef-86e0-002590c0647c</td>\n",
       "      <td>0.053270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37299</th>\n",
       "      <td>92106252-16fa-11eb-86e0-002590c0647c</td>\n",
       "      <td>0.053248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    user_id      pred\n",
       "50224  a046199a-9f61-11ea-a0d7-002590e45c38  0.893693\n",
       "81700  4814fc56-f93c-11eb-a6e9-002590c82437  0.886025\n",
       "74487  81e3ac7e-436d-11ef-86e0-002590c0647c  0.878027\n",
       "87929  101762d6-7733-11ee-bbb1-002590c82437  0.877128\n",
       "80924  99f0590a-bea9-11ee-bbb1-002590c82436  0.874505\n",
       "...                                     ...       ...\n",
       "80705  8f8056b2-2f02-11eb-86e0-002590c0647c  0.053368\n",
       "62219  e42c4da4-22f0-11ed-a044-002590c82437  0.053361\n",
       "3019   890e8a10-9a70-11ee-86e0-002590c0647c  0.053286\n",
       "14130  cfb1e80e-46a5-11ef-86e0-002590c0647c  0.053270\n",
       "37299  92106252-16fa-11eb-86e0-002590c0647c  0.053248\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Выбираем 3000 пользователей по наибольшей вероятности\n",
    "model, X_test, X_test_save = train_model(load_features_data())\n",
    "\n",
    "X_test_save = X_test_save.reset_index()\n",
    "\n",
    "X_test['pred'] = model.predict(X_test)\n",
    "X_test = X_test.reset_index()\n",
    "\n",
    "merged_users = X_test.merge(X_test_save, on='index')\n",
    "sorted_users = merged_users.sort_values('pred', ascending=False).head(3000)\n",
    "sorted_users[['user_id', 'pred']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ccb4e-936d-4a56-b94f-471015566871",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Optuna for binary classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187492eb-d525-4bcb-8d2e-647c597aef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "\n",
    "# # Загрузка данных\n",
    "# def load_features_data():\n",
    "#     features = pd.read_parquet('user_features.pq')\n",
    "#     return features\n",
    "\n",
    "# # Функция для обучения модели\n",
    "# def train_model(X_train, X_test, y_train, y_test, trial):\n",
    "#     # Определение пространства гиперпараметров для оптимизации\n",
    "#     params = {\n",
    "#         'objective': 'binary',\n",
    "#         'metric': 'binary_logloss',\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Логарифмическая сетка\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 12),  # Ограничение по глубине дерева\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#     }\n",
    "    \n",
    "#     # Создание данных для обучения и валидации\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train)\n",
    "#     valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "    \n",
    "#     # Обучение модели\n",
    "#     model = lgb.train(params, train_data, valid_sets=[valid_data], num_boost_round=100)\n",
    "    \n",
    "#     # Предсказания и метрики\n",
    "#     y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "#     return precision, recall\n",
    "\n",
    "# # Функция оптимизации\n",
    "# def objective(trial):\n",
    "#     features = load_features_data()\n",
    "    \n",
    "#     # Целевая переменная: наличие заказов\n",
    "#     features = features.drop('user_id', axis=1)\n",
    "#     X = features.drop(columns=['n_orders'])  # Признаки\n",
    "#     y = (features['n_orders'] > 0).astype(int)  # Купил ли товар\n",
    "    \n",
    "#     # Разделение на тренировочные и тестовые данные\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Обучение модели с текущими гиперпараметрами\n",
    "#     precision, recall = train_model(X_train, X_test, y_train, y_test, trial)\n",
    "    \n",
    "#     # Оптимизируем по F1-мере (или precision/recall, в зависимости от задачи)\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "#     return f1_score\n",
    "\n",
    "# # Оптимизация гиперпараметров с использованием Optuna\n",
    "# def optimize_params():\n",
    "#     # Создание объекта оптимизации\n",
    "#     study = optuna.create_study(direction='maximize')  # Мы максимизируем F1-меру\n",
    "#     study.optimize(objective, n_trials=1000)  # Оптимизируем на 50 итерациях (можно увеличить для лучшего результата)\n",
    "    \n",
    "#     print(f\"Best trial: {study.best_trial.params}\")\n",
    "#     return study.best_trial.params\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Поиск оптимальных гиперпараметров\n",
    "#     best_params = optimize_params()\n",
    "    \n",
    "#     # Загрузка данных с уже найденными оптимальными параметрами\n",
    "#     features = load_features_data()\n",
    "    \n",
    "#     # Разделение на признаки и целевую переменную\n",
    "#     features = features.drop('user_id', axis=1)\n",
    "#     X = features.drop(columns=['n_orders'])\n",
    "#     y = (features['n_orders'] > 0).astype(int)\n",
    "    \n",
    "#     # Разделение на тренировочные и тестовые данные\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Использование оптимальных параметров для обучения модели\n",
    "#     model = lgb.train(best_params, lgb.Dataset(X_train, label=y_train), valid_sets=[lgb.Dataset(X_test, label=y_test)], num_boost_round=100)\n",
    "    \n",
    "#     # Предсказания и вывод метрик\n",
    "#     y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    \n",
    "#     # Сохранение модели\n",
    "#     model.save_model('optimized_lgb_classifier.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1d613-7229-4c2c-9288-b351b9b72176",
   "metadata": {},
   "source": [
    "# Модель ранжирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194e0f24-9ae6-433f-b13c-cd621a175104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8b5a76-48e6-4213-a64a-1a9ddca3b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "catalog = pd.read_parquet('stokman_catalog_preprocessed.pq')\n",
    "actions = pd.read_parquet('train_actions.pq')\n",
    "catalog_vector_mapping = pd.read_parquet('catalog_vector_mapping.pq')\n",
    "vectors = np.load('vectors.npz')['arr_0']  # Извлечение эмбеддингов товаров\n",
    "\n",
    "# Преобразование даты в формате datetime\n",
    "actions['date'] = pd.to_datetime(actions['date'])\n",
    "catalog['add_date'] = pd.to_datetime(catalog['add_date'])\n",
    "\n",
    "# Пример слияния данных по product_id\n",
    "catalog = catalog.merge(catalog_vector_mapping, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa83366b-cb9e-4741-b5bd-b443e1999b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6580936/6580936 [01:02<00:00, 104953.82it/s]\n"
     ]
    }
   ],
   "source": [
    "products_counter = {}\n",
    "for i in tqdm(range(0, actions.shape[0])):\n",
    "    prod_array = actions.iat[i, 4]\n",
    "    for i in prod_array:\n",
    "        if i in products_counter:\n",
    "            products_counter[i] += 1\n",
    "        else:\n",
    "            products_counter[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dec8e06e-edfe-489e-a102-1e98b054c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity(x):\n",
    "    try:\n",
    "        return round(products_counter[x] / actions.shape[0] * 10**5, 2)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c88014-be25-4dd2-af55-8fc4c789dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_generation(catalog_data):\n",
    "    catalog_data['price_diff'] = 1 - catalog_data['price'] / catalog_data['old_price']\n",
    "    catalog_data['popularity'] = catalog_data['product_id'].apply(popularity)\n",
    "    return catalog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ab1715-b003-42ab-bae2-8af4a2d3a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = future_generation(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b6ae23f-f9aa-4969-90d2-22276307fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Развернем массивы product_id в отдельные строки\n",
    "actions = actions.explode('products')\n",
    "\n",
    "# Переименуем колонку для удобства\n",
    "actions = actions.rename(columns={'products': 'product_id'})\n",
    "\n",
    "user_item_interactions = actions.groupby(['user_id', 'product_id']).agg(\n",
    "    views=('action', lambda x: (x == 0).sum()),\n",
    "    likes=('action', lambda x: (x == 1).sum()),\n",
    "    add_to_cart=('action', lambda x: (x == 2).sum()),\n",
    "    orders=('action', lambda x: (x == 5).sum()),\n",
    "    last_action_time=('date', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Считаем общее количество действий пользователя\n",
    "user_features = actions.groupby('user_id').agg(\n",
    "    total_actions=('action', 'count'),\n",
    "    total_orders=('action', lambda x: (x == 5).sum()),\n",
    "    total_views=('action', lambda x: (x == 0).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Признаки товара\n",
    "item_features = actions.groupby('product_id').agg(\n",
    "    total_views=('action', lambda x: (x == 0).sum()),\n",
    "    total_add_to_cart=('action', lambda x: (x == 2).sum()),\n",
    "    total_orders=('action', lambda x: (x == 5).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Объединяем пользовательские и товарные признаки\n",
    "data = user_item_interactions.merge(user_features, on='user_id', how='left')\n",
    "data = data.merge(item_features, on='product_id', how='left')\n",
    "data = data.merge(catalog[['product_id', 'category_id', 'price', 'old_price', 'price_diff', 'popularity']], on='product_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c197704-cd82-4d70-9ee3-aef51832599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_18712\\3038899646.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['target'] = (train_data['orders'] > 0).astype(int)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_18712\\3038899646.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['target'] = (test_data['orders'] > 0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Предположим, что у нас есть временной промежуток для обучения и тестирования\n",
    "train_end_date = pd.Timestamp('2024-09-21')\n",
    "test_start_date = pd.Timestamp('2024-09-22')\n",
    "\n",
    "# Делаем таргет (покупка или нет) на основе данных о заказах (action == 5)\n",
    "train_data = data[data['last_action_time'] <= train_end_date]\n",
    "train_data['target'] = (train_data['orders'] > 0).astype(int)\n",
    "\n",
    "# Тестовая выборка\n",
    "test_data = data[data['last_action_time'] >= test_start_date]\n",
    "test_data['target'] = (test_data['orders'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c16dcc6-062c-408d-a5a7-8520af6f25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Присоединяем эмбеддинги товаров\n",
    "catalog_vector_mapping = catalog_vector_mapping.set_index('vector_id')\n",
    "vectors = pd.DataFrame(vectors)\n",
    "catalog_mapping = catalog_vector_mapping.merge(vectors, left_index=True, right_index=True)\n",
    "\n",
    "train_data = train_data.merge(catalog_mapping, on='product_id', how='left')\n",
    "test_data = test_data.merge(catalog_mapping, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee4eb34c-bdf1-49ba-b2c0-07ffc5587e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.416379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 100033\n",
      "[LightGBM] [Info] Number of data points in the train set: 370637, number of used features: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 22:26:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'ranked_model' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'ranked_model'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.9918\n",
      "Validation Recall: 0.9873\n"
     ]
    }
   ],
   "source": [
    "# Подготовка данных для обучения\n",
    "train_data = pd.read_parquet('train_data.pq')\n",
    "test_data = pd.read_parquet('test_data.pq')\n",
    "\n",
    "train_data = train_data.fillna(0)\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "train_data['category_id'] = train_data['category_id'].astype(int) \n",
    "test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "\n",
    "# Подготовим данные для обучения\n",
    "X = train_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])  # Удаляем также 'orders' и 'target'\n",
    "y = train_data['target']  # Используем целевую переменную\n",
    "\n",
    "# Получаем список пользователей\n",
    "groups = train_data['user_id']\n",
    "\n",
    "# Создаем объект GroupShuffleSplit для корректного разбиения данных по пользователям\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Разделяем данные на тренировочный и валидационный наборы, гарантируя, что данные одного пользователя будут в одном наборе\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "# Подготовим тренировочные и валидационные данные\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "# Считаем размер групп для каждого набора\n",
    "train_groups = X_train.groupby('user_id').size().values\n",
    "val_groups = X_val.groupby('user_id').size().values\n",
    "\n",
    "X_train = X_train.drop('user_id', axis=1)\n",
    "X_val = X_val.drop('user_id', axis=1)\n",
    "# Создаем обучающие и валидационные наборы для LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)\n",
    "\n",
    "# Параметры для модели LambdaRank\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Параметры модели\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'learning_rate': 0.07,\n",
    "    'num_leaves': 49,\n",
    "    'min_data_in_leaf': 84,\n",
    "    'feature_fraction': 0.95,\n",
    "    'bagging_fraction': 0.93,\n",
    "    'bagging_freq': 3,\n",
    "    'lambda_l1': 0.0129,\n",
    "    'lambda_l2': 0.000137\n",
    "}\n",
    "\n",
    "# Начало трекинга\n",
    "with mlflow.start_run():\n",
    "    # Логирование параметров\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Обучение модели\n",
    "    ranker = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid'],\n",
    "    )\n",
    "\n",
    "    # Логирование модели\n",
    "    mlflow.lightgbm.log_model(ranker, artifact_path=\"lgbm_ranking_model\", registered_model_name='ranked_model')\n",
    "\n",
    "    # Предсказания на валидационных данных\n",
    "    y_val_pred = ranker.predict(X_val)\n",
    "\n",
    "    # Преобразуем предсказания в бинарные (используем порог 0.5)\n",
    "    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "    # Расчет метрик precision и recall на валидационных данных\n",
    "    precision_val = precision_score(y_val, y_val_pred_binary, average='weighted')\n",
    "    recall_val = recall_score(y_val, y_val_pred_binary, average='weighted')\n",
    "\n",
    "    # Логирование precision и recall на валидационных данных в MLflow\n",
    "    mlflow.log_metric(\"precision_val\", precision_val)\n",
    "    mlflow.log_metric(\"recall_val\", recall_val)\n",
    "\n",
    "    print(f'Validation Precision: {precision_val:.4f}')\n",
    "    print(f'Validation Recall: {recall_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c19be8ae-8b57-4020-8c82-9184b99d2f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@25: 0.0456\n",
      "Precision@25: 0.0020\n",
      "Recall@25: 0.0193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "test_data = pd.read_parquet('test_data.pq')\n",
    "test_data = test_data.fillna(0)\n",
    "test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "\n",
    "# Функция для расчета NDCG для каждой группы\n",
    "def calculate_ndcg(test_data, k):\n",
    "    ndcg_scores = []\n",
    "    # Для каждого пользователя\n",
    "    for user_id, group in test_data.groupby('user_id'):\n",
    "        y_true = group['target'].values  # истинные значения (релевантность)\n",
    "        y_score = group['pred'].values  # предсказанные оценки\n",
    "        if len(y_true) > 1:  # NDCG имеет смысл только для группы из нескольких товаров\n",
    "            ndcg = ndcg_score([y_true], [y_score], k=k)\n",
    "            ndcg_scores.append(ndcg)\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "# Функция для расчета Precision@K и Recall@K\n",
    "def precision_recall_at_k(test_data, k):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    # Для каждого пользователя\n",
    "    for user_id, group in test_data.groupby('user_id'):\n",
    "        # Сортируем по предсказанным оценкам\n",
    "        group = group.sort_values('pred', ascending=False)\n",
    "        y_true = group['target'].values  # истинные метки\n",
    "        y_pred = group['pred'].values  # предсказанные метки\n",
    "\n",
    "        # Рассматриваем топ-K\n",
    "        top_k = y_true[:k]\n",
    "        relevant_items = np.sum(y_true)  # общее количество релевантных товаров\n",
    "\n",
    "        precision = np.sum(top_k) / k  # Precision@K\n",
    "        recall = np.sum(top_k) / relevant_items if relevant_items > 0 else 0  # Recall@K\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "    return np.mean(precision_scores), np.mean(recall_scores)\n",
    "\n",
    "# Логирование NDCG@K, Precision@K и Recall@K\n",
    "K = 25  # Топ-K для Precision и Recall\n",
    "\n",
    "# Подготовка тестовых данных\n",
    "X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target', 'user_id'])  # Удаляем лишние столбцы\n",
    "y_test = test_data['target']  # Истинные метки для тестовых данных\n",
    "\n",
    "# Предсказания для тестового набора\n",
    "test_data['pred'] = ranker.predict(X_test)\n",
    "\n",
    "# Вычисление NDCG@K\n",
    "mean_ndcg = calculate_ndcg(test_data, k=K)\n",
    "print(f'NDCG@{K}: {mean_ndcg:.4f}')\n",
    "\n",
    "# Вычисление Precision@K и Recall@K\n",
    "mean_precision, mean_recall = precision_recall_at_k(test_data, k=K)\n",
    "print(f'Precision@{K}: {mean_precision:.4f}')\n",
    "print(f'Recall@{K}: {mean_recall:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "797b7704-ac37-4dbc-88b1-0a3dbd86f7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5168833, 5168824, 5168806]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5422963, 6612845, 6715845, 6538627, 6604130, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6274777]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[6078485]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[6269449]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                product_id\n",
       "user_id                                                   \n",
       "0                              [5168833, 5168824, 5168806]\n",
       "1        [5422963, 6612845, 6715845, 6538627, 6604130, ...\n",
       "2                                                [6274777]\n",
       "3                                                [6078485]\n",
       "4                                                [6269449]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Предполагается, что модель ranker уже обучена и тестовые данные загружены в test_data\n",
    "# Идентификаторы пользователей и товаров также содержатся в test_data\n",
    "\n",
    "# Прогнозирование вероятности для тестового набора данных\n",
    "def get_top_25_relevant_products(test_data, model):\n",
    "    # Предсказания для тестового набора\n",
    "    X_test = test_data.drop(columns=['user_id', 'product_id', 'last_action_time', 'orders', 'target'])\n",
    "    test_data['pred'] = model.predict(X_test)\n",
    "    \n",
    "    # Сортируем данные по 'user_id' и предсказанным значениям ('pred')\n",
    "    test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=[True, False])\n",
    "    \n",
    "    # Для каждого пользователя выбираем топ-25 товаров\n",
    "    top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "    # Создаем новый датафрейм с 'user_id' и списком 25 наиболее релевантных 'product_id'\n",
    "    top_25_relevant_products = top_25.groupby('user_id').agg({'product_id': lambda x: list(x)})\n",
    "    \n",
    "    return top_25_relevant_products\n",
    "\n",
    "# Пример вызова функции\n",
    "test_data = pd.read_parquet('test_data.pq')\n",
    "test_data = test_data.fillna(0)\n",
    "test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "\n",
    "top_25_products_per_user = get_top_25_relevant_products(test_data, ranker)\n",
    "\n",
    "# Выводим результаты\n",
    "top_25_products_per_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be09b8-ac12-42f9-a271-55e3064a527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import GroupShuffleSplit\n",
    "# from sklearn.metrics import ndcg_score\n",
    "\n",
    "# # Функция для обучения и оценки модели\n",
    "# def objective(trial):\n",
    "#     train_data = pd.read_parquet('train_data.pq')\n",
    "#     test_data = pd.read_parquet('test_data.pq')\n",
    "    \n",
    "#     train_data = train_data.fillna(0)\n",
    "#     test_data = test_data.fillna(0)\n",
    "    \n",
    "#     train_data['category_id'] = train_data['category_id'].astype(int) \n",
    "#     test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "#     # Гиперпараметры, которые мы будем оптимизировать\n",
    "#     params = {\n",
    "#         'objective': 'lambdarank',\n",
    "#         'metric': 'ndcg',\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True), \n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "#         'lambda_l1': trial.suggest_float('lambda_l1', 1e-4, 1.0, log=True),\n",
    "#         'lambda_l2': trial.suggest_float('lambda_l2', 1e-4, 1.0, log=True)\n",
    "#     }\n",
    "\n",
    "#     # Разделяем данные на тренировочные и валидационные\n",
    "#     gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#     train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "#     X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "#     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "#     train_groups = X_train.groupby('user_id').size().values\n",
    "#     val_groups = X_val.groupby('user_id').size().values\n",
    "\n",
    "#     # Создаем обучающие и валидационные наборы для LightGBM\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "#     val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)\n",
    "\n",
    "#     # Обучение модели\n",
    "#     ranker = lgb.train(\n",
    "#         params,\n",
    "#         train_data,\n",
    "#         num_boost_round=100,\n",
    "#         valid_sets=[train_data, val_data],\n",
    "#         valid_names=['train', 'valid'],\n",
    "#     )\n",
    "    \n",
    "#     K = 25  # Топ-K для Precision и Recall\n",
    "\n",
    "#     # Подготовка тестовых данных\n",
    "#     X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])  # Удаляем лишние столбцы\n",
    "#     y_test = test_data['target']  # Истинные метки для тестовых данных\n",
    "    \n",
    "#     # Предсказания для тестового набора\n",
    "#     test_data['pred'] = ranker.predict(X_test)\n",
    "    \n",
    "#     # Вычисление NDCG@K\n",
    "#     mean_ndcg = calculate_ndcg(test_data, k=K)\n",
    "#     print(f'NDCG@{K}: {mean_ndcg:.4f}')\n",
    "\n",
    "#     return mean_ndcg\n",
    "    \n",
    "\n",
    "# # Создание объекта исследования Optuna\n",
    "# study = optuna.create_study(direction='maximize')  # Максимизируем NDCG\n",
    "# study.optimize(objective, n_trials=50)  # Запускаем 50 испытаний\n",
    "\n",
    "# # Выводим результаты\n",
    "# print(\"Best hyperparameters: \", study.best_params)\n",
    "# print(\"Best Recall: \", study.best_value)\n",
    "\n",
    "# ##В качестве метрики сделать массив со всеми купленными пользователем товарами, так же седалть массив с топ 25 товарами по предсказанию \n",
    "# ##По этим двум столбцам считать recall типо сколько купленных товаров в нашем предсказанном массиве и делаить на общее число покупок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6889a-aced-4a29-a3f0-fefdeb7955b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiColumnLabelEncoder:\n",
    "\n",
    "#     def __init__(self, columns=None):\n",
    "#         self.columns = columns # array of column names to encode\n",
    "\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         self.encoders = {}\n",
    "#         columns = X.columns if self.columns is None else self.columns\n",
    "#         for col in columns:\n",
    "#             self.encoders[col] = LabelEncoder().fit(X[col])\n",
    "#         return self\n",
    "\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         output = X.copy()\n",
    "#         columns = X.columns if self.columns is None else self.columns\n",
    "#         for col in columns:\n",
    "#             output[col] = self.encoders[col].transform(X[col])\n",
    "#         return output\n",
    "\n",
    "\n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         return self.fit(X,y).transform(X)\n",
    "\n",
    "\n",
    "#     def inverse_transform(self, X):\n",
    "#         output = X.copy()\n",
    "#         columns = X.columns if self.columns is None else self.columns\n",
    "#         for col in columns:\n",
    "#             output[col] = self.encoders[col].inverse_transform(X[col])\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6af794-7cc3-4a1d-80f1-2b95da03c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_encode = ['user_id']\n",
    "\n",
    "# train_encoder = MultiColumnLabelEncoder(columns_to_encode)\n",
    "# train_data = train_encoder.fit_transform(train_data)\n",
    "\n",
    "# test_encoder = MultiColumnLabelEncoder(columns_to_encode)\n",
    "# test_data = test_encoder.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7d466-dcd0-4d8d-b6a5-2ef90c46e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Подготовка тестовых данных\n",
    "#     X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "#     test_data['pred'] = ranker.predict(X_test)\n",
    "\n",
    "#     # Ранжирование товаров для каждого пользователя\n",
    "#     test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "#     top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "#     # Группировка и сохранение результатов\n",
    "#     result = top_25.groupby('user_id').agg({\n",
    "#         'product_id': lambda x: list(x)\n",
    "#     }).reset_index()\n",
    "\n",
    "#     purchased_data = test_data[test_data['orders'] > 0]\n",
    "#     purchased_products = purchased_data.groupby('user_id').agg({\n",
    "#         'product_id': lambda x: list(x)\n",
    "#     }).reset_index()\n",
    "\n",
    "#     table = result.merge(purchased_products, on='user_id', how='left')\n",
    "#     table = table.fillna(0)\n",
    "\n",
    "#     # Функция для расчета recall\n",
    "#     def recall(table):\n",
    "#         recall_values = []\n",
    "#         for i in tqdm(range(0, table.shape[0])):\n",
    "#             preds = table.iat[i, 1]\n",
    "#             buys = table.iat[i, 2]\n",
    "#             if buys == 0:\n",
    "#                 continue\n",
    "#             intersect = list(set(preds) & set(buys))\n",
    "#             recall_values.append(len(intersect) / len(buys))\n",
    "\n",
    "#         return np.mean(recall_values)\n",
    "\n",
    "#     # Расчет метрики recall на тестовых данных\n",
    "#     recall_test = recall(table)\n",
    "\n",
    "#     # Логирование метрики recall на тестовых данных в MLflow\n",
    "#     mlflow.log_metric(\"recall_test\", recall_test)\n",
    "\n",
    "#     print(f'Test Recall: {recall_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25be58-5f95-4098-88c9-4287f8973465",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MLFlowTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c3488-0101-4228-980e-52e3b8fe6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Инициализируем клиента MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Название вашей модели (замените 'ranked_model' на нужное имя)\n",
    "model_name = \"ranked_model\"\n",
    "\n",
    "# Получаем информацию обо всех версиях модели\n",
    "model_versions = client.get_latest_versions(model_name)\n",
    "\n",
    "# Выводим информацию о стадиях каждой версии модели\n",
    "for version in model_versions:\n",
    "    print(f\"Version: {version.version}, Stage: {version.current_stage}, Status: {version.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede1332-a07b-4a92-8958-12287b5865bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Инициализируем клиента MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Название вашей модели\n",
    "model_name = \"ranked_model\"\n",
    "\n",
    "# Версия модели, которую нужно перевести в новую стадию (укажите версию вашей модели)\n",
    "model_version = 1  # Замените на актуальную версию\n",
    "\n",
    "# Новая стадия (например, \"Staging\" или \"Production\")\n",
    "new_stage = \"Production\"  # Может быть \"Staging\", \"Production\", или \"Archived\"\n",
    "\n",
    "# Переводим модель в новую стадию\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=new_stage\n",
    ")\n",
    "\n",
    "print(f\"Модель {model_name} версии {model_version} переведена в стадию {new_stage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250234dc-946d-4bef-9709-65f7c77ea925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "import pandas as pd\n",
    "\n",
    "# URI модели из MLflow Model Registry (например, \"models:/<model_name>/Production\")\n",
    "model_uri = \"models:/ranked_model/Production\"  # Замените на своё имя модели\n",
    "\n",
    "# Загрузка модели из MLflow\n",
    "model = mlflow.lightgbm.load_model(model_uri)\n",
    "\n",
    "# Подготовка тестового набора данных\n",
    "test_data = pd.read_parquet('test_data.pq')\n",
    "test_data = test_data.fillna(0)\n",
    "test_data['category_id'] = test_data['category_id'].astype(int)\n",
    "X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "\n",
    "# Использование загруженной модели для предсказания на тестовых данных\n",
    "test_data['pred'] = model.predict(X_test)\n",
    "\n",
    "# Ранжирование товаров для каждого пользователя по предсказанным оценкам\n",
    "test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "\n",
    "# Выбираем топ-25 товаров для каждого пользователя\n",
    "top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "# Преобразуем товары в массив для каждого пользователя\n",
    "result = top_25.groupby('user_id').agg({\n",
    "    'product_id': lambda x: list(x),  # Преобразуем товары в список\n",
    "    'pred': lambda x: list(x)  # Преобразуем предсказанные оценки в список (опционально)\n",
    "}).reset_index()\n",
    "\n",
    "# Фильтрация только купленных товаров (предполагается, что 'orders' > 0 означает покупку)\n",
    "purchased_data = test_data[test_data['orders'] > 0]\n",
    "\n",
    "# Группировка по user_id и преобразование списка купленных товаров в массив\n",
    "purchased_products = purchased_data.groupby('user_id').agg({\n",
    "    'product_id': lambda x: list(x)  # Преобразуем купленные товары в список\n",
    "}).reset_index()\n",
    "\n",
    "# Выводим результат\n",
    "purchased_products.columns = ['user_id', 'bought_products']\n",
    "\n",
    "# Объединение результатов\n",
    "table = result.merge(purchased_products, on='user_id', how='left')\n",
    "table = table.fillna(0).drop('pred', axis=1)\n",
    "\n",
    "# Функция для расчета recall\n",
    "def recall(table):\n",
    "    recall_values = []\n",
    "    for i in tqdm(range(0, table.shape[0])):\n",
    "        preds = table.iat[i, 1]\n",
    "        buys = table.iat[i, 2]\n",
    "        if buys == 0:\n",
    "            continue\n",
    "        intersect = list(set(preds) & set(buys))\n",
    "        recall_values.append(len(intersect) / len(buys))\n",
    "\n",
    "    return np.mean(recall_values)\n",
    "\n",
    "# Расчет recall на тестовых данных\n",
    "recall_test = recall(table)\n",
    "\n",
    "print(f'Test Recall: {recall_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda542c-1fc4-49b8-8f8b-af57aea52c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Предсказания на валидационных данных\n",
    "# y_val_pred = ranker.predict(X_val)\n",
    "\n",
    "# # Преобразуем предсказания в бинарный формат\n",
    "# y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "# # Расчет метрик точности\n",
    "# # Поскольку у нас многоклассовый случай, выбираем 'weighted'\n",
    "# precision = precision_score(y_val, y_val_pred_binary, average='weighted')\n",
    "# recall = recall_score(y_val, y_val_pred_binary, average='weighted')\n",
    "\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76766734-d55c-44bc-8f5a-2e31005e2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Подготовка тестового набора\n",
    "# X_test = test_data.drop(columns=['product_id', 'last_action_time', 'orders', 'target'])\n",
    "\n",
    "# # Предсказания для тестового набора\n",
    "# test_data['pred'] = ranker.predict(X_test)\n",
    "\n",
    "# # Ранжируем товары для каждого пользователя по предсказанным оценкам\n",
    "# test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "\n",
    "# # Выбираем топ-25 товаров для каждого пользователя\n",
    "# top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "\n",
    "# # Преобразуем товары в массив для каждого пользователя\n",
    "# result = top_25.groupby('user_id').agg({\n",
    "#     'product_id': lambda x: list(x),  # Преобразуем товары в список\n",
    "#     'pred': lambda x: list(x)  # Преобразуем предсказанные оценки в список (опционально)\n",
    "# }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e265c-11b7-4e2f-8e25-615c892d1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Фильтрация только купленных товаров (предполагается, что 'orders' > 0 означает покупку)\n",
    "# purchased_data = test_data[test_data['orders'] > 0]\n",
    "\n",
    "# # Группировка по user_id и преобразование списка купленных товаров в массив\n",
    "# purchased_products = purchased_data.groupby('user_id').agg({\n",
    "#     'product_id': lambda x: list(x)  # Преобразуем купленные товары в список\n",
    "# }).reset_index()\n",
    "\n",
    "# # Выводим результат\n",
    "# purchased_products.columns = ['user_id', 'bought_products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64051821-bac4-48ac-a7d4-8a9681c4f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = result.merge(purchased_products, on='user_id', how='left')\n",
    "# table = table.fillna(0).drop('pred', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3f441-f16b-462a-872b-5577fee76287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recall(table):\n",
    "#     recall = []\n",
    "#     for i in tqdm(range(0, table.shape[0])):\n",
    "#         preds = table.iat[i, 1]\n",
    "#         buys = table.iat[i, 2]\n",
    "#         if buys == 0:\n",
    "#             pass\n",
    "#         else:\n",
    "#             intersect = list(set(preds) & set(buys))\n",
    "#             recall.append(len(intersect) / len(buys))\n",
    "    \n",
    "    \n",
    "#     return np.mean(recall)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7873ee-b2e0-4da5-81c7-9811f6103fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = recall(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229eb85-8c35-452b-afc3-ac467416caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3698c52-1a65-4142-b121-13f949f54e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import GroupShuffleSplit\n",
    "# from sklearn.metrics import ndcg_score\n",
    "\n",
    "# # Функция для обучения и оценки модели\n",
    "# def objective(trial):\n",
    "#     # Гиперпараметры, которые мы будем оптимизировать\n",
    "#     params = {\n",
    "#         'objective': 'lambdarank',\n",
    "#         'metric': 'ndcg',\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True), \n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "#         'lambda_l1': trial.suggest_float('lambda_l1', 1e-4, 1.0, log=True),\n",
    "#         'lambda_l2': trial.suggest_float('lambda_l2', 1e-4, 1.0, log=True)\n",
    "#     }\n",
    "\n",
    "#     # Разделяем данные на тренировочные и валидационные\n",
    "#     gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#     train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "#     X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "#     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "#     train_groups = X_train.groupby('user_id').size().values\n",
    "#     val_groups = X_val.groupby('user_id').size().values\n",
    "\n",
    "#     # Создаем обучающие и валидационные наборы для LightGBM\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "#     val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)\n",
    "\n",
    "#     # Обучение модели\n",
    "#     ranker = lgb.train(\n",
    "#         params,\n",
    "#         train_data,\n",
    "#         num_boost_round=100,\n",
    "#         valid_sets=[train_data, val_data],\n",
    "#         valid_names=['train', 'valid'],\n",
    "#     )\n",
    "    \n",
    "#     # Предсказания для тестового набора\n",
    "#     test_data['pred'] = ranker.predict(X_test)\n",
    "    \n",
    "#     # Ранжируем товары для каждого пользователя по предсказанным оценкам\n",
    "#     test_data_sorted = test_data.sort_values(by=['user_id', 'pred'], ascending=False)\n",
    "    \n",
    "#     # Выбираем топ-25 товаров для каждого пользователя\n",
    "#     top_25 = test_data_sorted.groupby('user_id').head(25)\n",
    "    \n",
    "#     # Преобразуем товары в массив для каждого пользователя\n",
    "#     result = top_25.groupby('user_id').agg({\n",
    "#         'product_id': lambda x: list(x),  # Преобразуем товары в список\n",
    "#         'pred': lambda x: list(x)  # Преобразуем предсказанные оценки в список (опционально)\n",
    "#     }).reset_index()\n",
    "\n",
    "#     table = result.merge(purchased_products, on='user_id', how='left')\n",
    "#     table = table.fillna(0).drop('pred', axis=1)\n",
    "\n",
    "#     return recall(table)\n",
    "\n",
    "    \n",
    "\n",
    "# # Создание объекта исследования Optuna\n",
    "# study = optuna.create_study(direction='maximize')  # Максимизируем NDCG\n",
    "# study.optimize(objective, n_trials=50)  # Запускаем 50 испытаний\n",
    "\n",
    "# # Выводим результаты\n",
    "# print(\"Best hyperparameters: \", study.best_params)\n",
    "# print(\"Best Recall: \", study.best_value)\n",
    "\n",
    "# ##В качестве метрики сделать массив со всеми купленными пользователем товарами, так же седалть массив с топ 25 товарами по предсказанию \n",
    "# ##По этим двум столбцам считать recall типо сколько купленных товаров в нашем предсказанном массиве и делаить на общее число покупок"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944674b3-8c8f-4acd-b171-3d95c007de3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871087d-ddc5-499c-a16b-baae21a464ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import timedelta\n",
    "\n",
    "# Преобразование даты и извлечение признаков\n",
    "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Ничего не нужно обучать\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Преобразование даты\n",
    "        X['date'] = pd.to_datetime(X['date'])\n",
    "\n",
    "        # Присоединение каталога товаров\n",
    "        X = X.explode('products')  # Распаковка массива products\n",
    "        X = X.rename(columns={'products': 'product_id'})\n",
    "        # Пример присоединения других данных, пока используем фиктивные данные\n",
    "        X['price'] = np.random.rand(X.shape[0])  # Фиктивные данные\n",
    "        X['category_id'] = np.random.randint(1, 10, size=X.shape[0])  # Фиктивные данные\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "# Генерация признаков\n",
    "class FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Ничего не нужно обучать\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Время с последнего действия для каждого пользователя\n",
    "        X['days_since_last_action'] = X.groupby('user_id')['date'].diff().dt.days\n",
    "\n",
    "        # Признаки активности за последние 3 дня\n",
    "        def count_recent_actions(df, days):\n",
    "            recent = df[df['date'] >= df['date'].max() - pd.Timedelta(days=days)]\n",
    "            return recent.groupby('user_id')['action'].count()\n",
    "\n",
    "        recent_activity_3d = count_recent_actions(X, 3)\n",
    "\n",
    "        # Количество покупок, добавлений в корзину и просмотров\n",
    "        agg_features = X.groupby('user_id').agg({\n",
    "            'product_id': 'nunique',  # Количество уникальных товаров\n",
    "            'price': 'mean',          # Средняя цена товаров\n",
    "            'action': ['count', lambda x: (x == 5).sum()],  # Количество действий и покупок\n",
    "            'days_since_last_action': 'min'  # Время с последнего действия\n",
    "        }).reset_index()\n",
    "\n",
    "        agg_features.columns = ['user_id', 'n_unique_products', 'avg_price', 'n_actions', 'n_orders', 'days_since_last_action']\n",
    "\n",
    "        # Объединение с активностью за 3 дня\n",
    "        agg_features = agg_features.merge(recent_activity_3d, on='user_id', how='left')\n",
    "        agg_features = agg_features.rename(columns={'action': 'actions_last_3d'})\n",
    "\n",
    "        return agg_features\n",
    "\n",
    "\n",
    "# Преобразователь для работы с отсутствующими значениями\n",
    "class NullDataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Ничего не нужно обучать\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.fillna(0)  # Заполнение пропущенных значений нулями\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab2526-fa00-4cfe-8f8b-ab4b47d087f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score\n",
    "\n",
    "# Обучение модели LightGBM\n",
    "class LightGBMClassifier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', learning_rate=0.05, num_leaves=31)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "# Создание пайплайна\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', DataPreprocessor()),  # Предобработка данных\n",
    "    ('feature_generator', FeatureGenerator()),  # Генерация признаков\n",
    "    ('null_data', NullDataPreprocessor()),  # Предобработка пропущенных данных\n",
    "    ('classifier', LightGBMClassifier())  # Классификатор LightGBM\n",
    "])\n",
    "\n",
    "# Загрузка данных\n",
    "def load_features_data():\n",
    "    features = pd.read_parquet('user_features.pq')\n",
    "    return features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = load_features_data()\n",
    "\n",
    "    X = features.drop(columns=['n_orders', 'user_id'])  # Признаки\n",
    "    y = (features['n_orders'] > 0).astype(int)  # Целевая переменная\n",
    "\n",
    "    # Разделение на тренировочные и тестовые данные\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Обучение модели\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Предсказания\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Оценка качества модели\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
